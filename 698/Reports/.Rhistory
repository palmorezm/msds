library(summarytools)
library(tidyverse)
library(DataExplorer)
library(reshape2)
library(mice)
library(caret)
library(MASS)
library(e1071)
library(tree)
library(corrplot)
library(kableExtra)
library(htmltools)
library(readxl)
library(psych)
library(xgboost)
library(ParBayesianOptimization)
library(factoextra)
library(kernlab)
set.seed(622)
# read data
adhd_data <- read_excel("ADHD_data.xlsx", sheet = "Data") %>% na_if("") %>% dplyr::select(-1)
#columns <- list(dimnames(adhd_data)[2])
#df <- adhd_data[,2:53]
adhd_data[,2:53] <- lapply(adhd_data[,2:53], factor)
adhd_data.dims <- dim(adhd_data)
adhd_data[,c(23:37)]
# select categorical columns
cat_cols <- dimnames(adhd_data[,2:53])[[2]]
adhd_fact <-  adhd_data[cat_cols]
# long format
adhd_factm <- melt(adhd_fact, measure.vars = cat_cols, variable.name = 'metric', value.name = 'value')
# plot categorical columns
ggplot(adhd_factm, aes(x = value)) +
geom_bar(aes(fill = metric)) +
facet_wrap( ~ metric, nrow = 5L, scales = 'free') + coord_flip() +
theme(legend.position = "none")
dfSummary(adhd_data, style = 'grid', graph.col = FALSE)
adhds <- sapply(adhd_data[,c(4:21)], as.numeric) %>% cor()
corrplot::corrplot(adhds, method="number")
mds <- sapply(adhd_data[,c(23:37)], as.numeric) %>% cor()
corrplot::corrplot(mds, method="number")
adhd_ques_fa <- factanal(sapply(adhd_data[,c(4:21)], as.numeric),
factors = 3,
rotation = "promax",
scores = "regression")
adhd_ques_fa
fa.diagram(adhd_ques_fa$loadings)
md_ques_fa <- factanal(sapply(adhd_data[,c(23:37)], as.numeric),
factors = 3,
rotation = "promax",
scores = "regression")
md_ques_fa
fa.diagram(md_ques_fa$loadings)
# ADHD question scores dataframe
adhd_ques_fa <- as.data.frame(adhd_ques_fa$scores)
names(adhd_ques_fa) <- c('ADHD_FACT1','ADHD_FACT2','ADHD_FACT3')
# MD questions scores dataframe
md_ques_fa <- as.data.frame(md_ques_fa$scores)
names(md_ques_fa) <- c('MD_FACT1','MD_FACT2','MD_FACT3')
# remove ADHD and MD columns
adhd_newdata <- adhd_data %>% dplyr::select(-c(starts_with('ADHD Q'), starts_with('MD Q')))
# Add new factor columns created
adhd_newdata <- cbind(adhd_newdata, adhd_ques_fa, md_ques_fa)
head(adhd_newdata)
adhd_newdata.dims <- dim(adhd_newdata)
dim_reduced <- (53-25)/53
# plot missing values
plot_missing(adhd_newdata)
# rename columns to apply mice
adhd_newdata <- adhd_newdata %>%
rename('ADHD_Total'='ADHD Total',
'MD_Total'='MD TOTAL',
'Sedative_hypnotics'='Sedative-hypnotics',
'Court_order' = 'Court order',
'Hx_of_Violence'='Hx of Violence',
'Disorderly_Conduct'='Disorderly Conduct',
'Non_subst_Dx'='Non-subst Dx',
'Subst_Dx'='Subst Dx',
'Psych_meds'='Psych meds.') %>%
dplyr::select(-Psych_meds)
# select columns with non missing values
temp <- adhd_newdata %>% dplyr::select(c(starts_with('ADHD_'), starts_with('MD_'), 'Race', 'Sex', 'Age'))
# impute predictors using mice
adhd_impute <- adhd_newdata %>% dplyr::select(-c(starts_with('ADHD_'), starts_with('MD_'), 'Race', 'Sex', 'Age'))
adhd_impute <- complete(mice(data=adhd_impute, print=FALSE))
summary(adhd_impute)
# Merged the imputed dataframe with temp
adhd_newdata <- cbind(adhd_impute, temp)
head(adhd_newdata)
# Filter out
#adhd_data <- adhd_data %>% filter(!is.na(Alcohol) &
#                                  !is.na(THC) &
#                                  !is.na(Cocaine) &
#                                  !is.na(Stimulants) &
#                                  !is.na(`Sedative-hypnotics`) &
#                                  !is.na(Opioids) &
#                                  !is.na(`Court order`) &
#                                  !is.na(Education) &
#                                  !is.na(`Hx of Violence`) &
#                                  !is.na(`Disorderly Conduct`) &
#                                  !is.na(Suicide) &
#                                  !is.na(Abuse) &
#                                  !is.na(`Non-subst Dx`) &
#                                  !is.na(`Subst Dx`) &
#                                  !is.na(`Psych meds.`))
# impute numeric predictors using mice
#adhd_data <- complete(mice(data=adhd_data[,:53], method="pmm", print=FALSE))
set.seed(622)
# create dummy variables for categorical features
adhd_dummy <- dummyVars(Suicide ~ ., data = adhd_newdata)
adhd_dummy <- predict(adhd_dummy, newdata=adhd_newdata)
# center and scaling
adhd_transformed <- adhd_dummy %>%
preProcess(c("center", "scale")) %>%
predict(adhd_dummy) %>%
as.data.frame()
# add Suicide column
adhd_transformed$Suicide <- adhd_newdata$Suicide
head(adhd_transformed)
set.seed(622)
partition <- createDataPartition(adhd_data$Suicide, p=0.75, list = FALSE)
training <- adhd_data[partition,]
testing <- adhd_data[-partition,]
# training/validation partition for independent variables
#X.train <- ld.clean[partition, ] %>% dplyr::select(-Loan_Status)
#X.test <- ld.clean[-partition, ] %>% dplyr::select(-Loan_Status)
# training/validation partition for dependent variable Loan_Status
#y.train <- ld.clean$Loan_Status[partition]
#y.test <- ld.clean$Loan_Status[-partition]
# create subset of ADHD Questions for PCA
adhd_ques_pca <- sapply(adhd_data[,c(4:21)], as.numeric)
# create subset of MD Questions for PCA
md_ques_pca <- sapply(adhd_data[,c(23:37)], as.numeric)
pca_adhd <- prcomp(adhd_ques_pca, scale. = TRUE, center=TRUE)
cor(adhd_ques_pca, pca_adhd$x[,1:10]) %>%
kableExtra::kbl(booktabs = T, caption ="ADHD Correlations") %>%
kable_styling(latex_options = c("striped"), full_width = F)
summary(pca_adhd)
fviz_eig(pca_adhd)
#top 10 contributors to the dimension of PC1 and PC2
fviz_contrib(pca_adhd, choice = "var", axes = c(1,2), top = 15)
fviz_pca_var(pca_adhd,
col.var ="contrib", # Color by contributions to the PC
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE     # Avoid text overlapping
,axes=c(1,2)
)
#top 10 contributors to the dimension of PC1 and PC3
fviz_contrib(pca_adhd, choice = "var", axes = c(1,3), top = 15)
fviz_pca_var(pca_adhd,
col.var = "contrib", # Color by contributions to the PC
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE     # Avoid text overlapping
,axes=c(1,3)
)
#top 10 contributors to the dimension of PC2 and PC3
fviz_contrib(pca_adhd, choice = "var", axes = c(2,3), top = 15)
fviz_pca_var(pca_adhd,
col.var = "contrib", # Color by contributions to the PC
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE     # Avoid text overlapping
,axes=c(2,3)
)
pca_md <- prcomp(md_ques_pca, scale. = TRUE, center=TRUE)
cor(md_ques_pca, pca_md$x[,1:10]) %>%
kableExtra::kbl(booktabs = T, caption ="md Correlations") %>%
kable_styling(latex_options = c("striped"), full_width = F)
summary(pca_md)
fviz_eig(pca_md)
#top 10 contributors to the dimension of PC1 and PC2
fviz_contrib(pca_md, choice = "var", axes = c(1,2), top = 15)
fviz_pca_var(pca_md,
col.var ="contrib", # Color by contributions to the PC
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE     # Avoid text overlapping
,axes=c(1,2)
)
#top 10 contributors to the dimension of PC1 and PC3
fviz_contrib(pca_md, choice = "var", axes = c(1,3), top = 15)
fviz_pca_var(pca_md,
col.var = "contrib", # Color by contributions to the PC
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE     # Avoid text overlapping
,axes=c(1,3)
)
#top 10 contributors to the dimension of PC2 and PC3
fviz_contrib(pca_md, choice = "var", axes = c(2,3), top = 15)
fviz_pca_var(pca_md,
col.var = "contrib", # Color by contributions to the PC
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE     # Avoid text overlapping
,axes=c(2,3)
)
gb__train <-subset(training[complete.cases(training$Suicide), ], select= -`Non-subst Dx`)
gb__test <-subset(testing[complete.cases(testing$Suicide), ], select= -`Non-subst Dx`)
y_label_tr <- as.matrix(gb__train$Suicide)
y_label_test <- as.matrix(gb__test$Suicide)
gb__train <- sapply(subset(gb__train, select = -Suicide), as.numeric)
gb_test <- sapply(subset(gb__test, select = -Suicide), as.numeric)
Folds <- list(
Fold1 = as.integer(seq(1,nrow(gb__train),by = 3))
, Fold2 = as.integer(seq(2,nrow(gb__train),by = 3))
, Fold3 = as.integer(seq(3,nrow(gb__train),by = 3))
)
scoringFunction <- function(max_depth, min_child_weight, subsample) {
dtrain <- xgb.DMatrix(gb__train, label=y_label_tr)
Pars <- list(
booster = "gbtree"
, eta = 0.01
, max_depth = max_depth
, min_child_weight = min_child_weight
, subsample = subsample
, objective = "binary:logistic"
, eval_metric = "auc"
)
xgbcv <- xgb.cv(
params = Pars
, data = dtrain
, nround = 100
, folds = Folds
, prediction = TRUE
, showsd = TRUE
, early_stopping_rounds = 5
, maximize = TRUE
, verbose = 0)
return(
list(
Score = max(xgbcv$evaluation_log$test_auc_mean)
, nrounds = xgbcv$best_iteration
)
)
}
set.seed(50)
bounds <- list(
max_depth = c(2L, 10L)
, min_child_weight = c(1, 25)
, subsample = c(0.25, .5)
)
optObj <- bayesOpt(
FUN = scoringFunction
, bounds = bounds
, initPoints = 4
, iters.n = 3
)
optObj$scoreSummary
print(getBestPars(optObj))
set.seed(622)
mode <- function(x){
levels <- unique(x)
indicies <- tabulate(match(x, levels))
levels[which.max(indicies)]
}
# Clean up training data
training_factors <- training %>%
dplyr::select(-Age, -`ADHD Total`, `MD TOTAL`)
training_factors <- data.frame(lapply(training_factors, as.factor))
train_knn <- training_factors %>%
mutate(across(everything(), ~replace_na(., mode(.))))
mode(train_knn$Psych.meds.)
train_knn$Psych.meds.[which(is.na(train_knn$Psych.meds.))] <- 0
sum(is.na(train_knn$Psych.meds.))
# Clean up testing data
testing_factors <- testing %>%
dplyr::select(-Age, -`ADHD Total`, `MD TOTAL`)
testing_factors <- data.frame(lapply(testing_factors, as.factor))
test_knn <- testing_factors %>%
mutate(across(everything(), ~replace_na(., mode(.))))
mode(test_knn$Psych.meds.)
test_knn$Psych.meds.[which(is.na(test_knn$Psych.meds.))] <- 0
sum(is.na(test_knn$Psych.meds.))
# Train KNN model
train.knn <- (train_knn[, names(train_knn) != "Suicide"])
prep <- preProcess(x = train.knn, method = c("center", "scale"))
cl <- trainControl(method="repeatedcv", repeats = 5)
knn_model <- train(Suicide ~ ., data = train_knn,
method = "knn",
trControl = cl,
preProcess = c("center","scale"),
tuneLength = 20)
knn_model
# Evaluate Model
plot(knn_model)
knn_predict <- predict(knn_model, newdata = test_knn)
mean(knn_predict == test_knn$Suicide) # accuracy
conf.mat.knn <- confusionMatrix(knn_predict, test_knn$Suicide)
accuracy <- round(conf.mat.knn$overall[[1]], 3)*100
conf.mat.knn
set.seed(622)
# Clean up training data
training_factors <- training %>%
na.omit() %>%
dplyr::select(-Age, -`ADHD Total`, -`MD TOTAL`)
training_factors <- data.frame(lapply(training_factors, as.factor))
train_heir <- training_factors %>%
mutate(across(everything(), ~replace_na(., mode(.))))
# mode(train_heir$Psych.meds.)
# train_heir$Psych.meds.[which(is.na(train_heir$Psych.meds.))]
# sum(is.na(train_heir))
# Clean up testing data
testing_factors <- testing %>%
na.omit() %>%
dplyr::select(-Age, -`ADHD Total`, -`MD TOTAL`)
testing_factors <- data.frame(lapply(testing_factors, as.factor))
test_heir <- testing_factors %>%
mutate(across(everything(), ~replace_na(., mode(.))))
# mode(test_heir$Psych.meds.)
# test_heir$Psych.meds.[which(is.na(test_heir$Psych.meds.))]
# sum(is.na(test_heir$Psych.meds.))
# Train KNN model
train.heir <- (train_heir[, names(train_heir) != "Suicide"])
prep <- preProcess(x = train.heir, method = c("center", "scale"))
cl <- trainControl(method="repeatedcv", repeats = 5)
heir_model <- train(Suicide ~ ., data = train_heir,
method = "knn",
trControl = cl,
preProcess = c("center","scale"),
tuneLength = 20)
heir_model
# Evaluate Model
plot(heir_model)
heir_predict <- predict(heir_model, newdata = test_heir)
conf.mat.heir <- confusionMatrix(heir_predict, test_heir$Suicide)
accuracy <- round(conf.mat.heir$overall[[1]], 3)*100
# partitioning for train and test
partition <- createDataPartition(adhd_transformed$Suicide, p=0.75, list = FALSE)
training <- adhd_transformed[partition,]
testing <- adhd_transformed[-partition,]
set.seed(622)
# fit with svmLinear
svm_lin_fit <- train(Suicide ~ .,
data = training,
method = "svmLinear",
preProcess = c("center","scale"),
tuneLength = 5,
trControl = trainControl(method = "cv"))
pred_lin_suicide <- predict(svm_lin_fit, testing)
cm_lin <- confusionMatrix(testing$Suicide, pred_lin_suicide)
# fit with svmRadial
svm_rad_fit <- train(Suicide ~ .,
data = training,
method = "svmRadial",
preProcess = c("center","scale"),
tuneLength = 5,
trControl = trainControl(method = "cv"))
pred_rad_suicide <- predict(svm_rad_fit, testing)
cm_rad <- confusionMatrix(testing$Suicide, pred_rad_suicide)
# fit with svmPoly
svm_poly_fit <- train(Suicide ~ .,
data = training,
method = "svmPoly",
preProcess = c("center","scale"),
tuneLength = 5,
trControl = trainControl(method = "cv"))
pred_poly_suicide <- predict(svm_poly_fit, testing)
cm_poly <- confusionMatrix(testing$Suicide, pred_poly_suicide)
# Evaluate models
conf.mat.svnpoly <- confusionMatrix(pred_poly_suicide, testing$Suicide)
conf.mat.svnrad <- confusionMatrix(pred_rad_suicide, testing$Suicide)
conf.mat.svnlin <- confusionMatrix(pred_lin_suicide, testing$Suicide)
#Compare 3 models:
svm_resamps <- resamples(list(Linear = svm_lin_fit, Radial = svm_rad_fit, Poly = svm_poly_fit))
summary(svm_resamps)
# Important features
imp_vars <- varImp(svm_rad_fit, scale=FALSE)
plot(imp_vars, top=10)
dtrain <- xgb.DMatrix(gb__train, label=y_label_tr)
dtest <- xgb.DMatrix(gb_test, label=y_label_test)
xgb <- xgb.train(
params = list(
booster = "gbtree"
, eta = 0.01
, max_depth = 10
, min_child_weight = 1
, subsample = .5
, objective = "binary:logistic"
, eval_metric = "auc"
)
, data = dtrain
, nround = 100
, maximize = TRUE
, verbose = 0)
xgbpred <- predict(xgb,dtest)
xgbpred <- ifelse (xgbpred > 0.5,1,0)
y_label_test <- as.numeric(y_label_test)
conf.mat.xgboost <- confusionMatrix(table(xgbpred, y_label_test))
conf.mat.xgboost
(round(data.frame(conf.mat.knn$overall,
conf.mat.heir$overall,
conf.mat.svnlin$overall,
conf.mat.svnrad$overall,
conf.mat.svnpoly$overall,
conf.mat.xgboost$overall), 3)) %>%
kbl(booktabs =  T, caption = "Model Performance") %>%
kable_styling(latex_options = c("striped, HODL_position"), full_width = F) %>%
footnote(c(
"Variables from PCA & factor analysis considered in model performance evaluation"
))
(round(data.frame(conf.mat.knn$overall,
conf.mat.heir$overall,
conf.mat.svnlin$overall,
conf.mat.svnrad$overall,
conf.mat.svnpoly$overall,
conf.mat.xgboost$overall), 3)) %>%
kbl(booktabs =  T, caption = "Model Performance") %>%
kable_styling(latex_options = c("striped, HOLD_position"), full_width = F) %>%
footnote(c(
"Variables from PCA & factor analysis considered in model performance evaluation"
))
(round(data.frame(conf.mat.knn$overall,
conf.mat.heir$overall,
conf.mat.svnlin$overall,
conf.mat.svnrad$overall,
conf.mat.svnpoly$overall,
conf.mat.xgboost$overall), 3))
set.seed(622)
# Clean up training data
training_factors <- training %>%
na.omit() %>%
dplyr::select(-Age, -`ADHD Total`, -`MD TOTAL`)
# Create Euclidean Dissimilarity Matrix
d <- dist(train.heir, method = "euclidean")
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )
# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang = -1)
# Cut tree into 4 groups
sub_grp <- cutree(hc1, k = 4)
table(sub_grp)
train.heir %>%
mutate(cluster = sub_grp) %>%
train.heir %>%
mutate(cluster = sub_grp)
train.heir %>%
mutate(cluster = sub_grp) %>% head()
plot(hc1, cex = 0.6)
rect.hclust(hc1, k = 4, border = 2:5)
plot(hc1, cex = 0.6)
rect.hclust(hc1, k = 4, border = 2:5)
plot(hc1, cex = 0.6)
rect.hclust(hc1, k = 25, border = 2:5)
sub_grp <- cutree(hc1, k = 24)
train.heir %>%
mutate(cluster = sub_grp) %>%
head()
plot(hc1, cex = 0.6)
rect.hclust(hc1, k = 24, border = 2:5)
sub_grp <- cutree(hc1, k = 3)
train.heir %>%
mutate(cluster = sub_grp) %>%
head()
plot(hc1, cex = 0.6)
rect.hclust(hc1, k = 3, border = 2:5)
(round(data.frame(conf.mat.knn$overall,
conf.mat.heir$overall,
conf.mat.svnlin$overall,
conf.mat.svnrad$overall,
conf.mat.svnpoly$overall,
conf.mat.xgboost$overall), 3))  %>%
rename(conf.mat.knn.overall = KNN,
conf.mat.heir.overall = HC,
conf.mat.svnlin.overall = LIN,
conf.mat.svnrad.overall = RAD,
conf.mat.svnpoly.overall = PLY,
conf.mat.xgboost.overall = XGB)
(round(data.frame(conf.mat.knn$overall,
conf.mat.heir$overall,
conf.mat.svnlin$overall,
conf.mat.svnrad$overall,
conf.mat.svnpoly$overall,
conf.mat.xgboost$overall), 3))
(round(data.frame(conf.mat.knn$overall,
conf.mat.heir$overall,
conf.mat.svnlin$overall,
conf.mat.svnrad$overall,
conf.mat.svnpoly$overall,
conf.mat.xgboost$overall), 3)) %>%
rename(KNN = conf.mat.knn.overall,
conf.mat.heir.overall = HC,
conf.mat.svnlin.overall = LIN,
conf.mat.svnrad.overall = RAD,
conf.mat.svnpoly.overall = PLY,
conf.mat.xgboost.overall = XGB)
(round(data.frame(conf.mat.knn$overall,
conf.mat.heir$overall,
conf.mat.svnlin$overall,
conf.mat.svnrad$overall,
conf.mat.svnpoly$overall,
conf.mat.xgboost$overall), 3)) %>%
rename(KNN = conf.mat.knn.overall,
HC = conf.mat.heir.overall,
LIN = conf.mat.svnlin.overall,
RAD = conf.mat.svnrad.overall,
PLY = conf.mat.svnpoly.overall,
XGB = conf.mat.xgboost.overall)
(round(data.frame(conf.mat.knn$overall,
conf.mat.heir$overall,
conf.mat.svnlin$overall,
conf.mat.svnrad$overall,
conf.mat.svnpoly$overall,
conf.mat.xgboost$overall), 3)) %>%
rename(KNN = conf.mat.knn.overall,
HC = conf.mat.heir.overall,
LIN = conf.mat.svnlin.overall,
RAD = conf.mat.svnrad.overall,
PLY = conf.mat.svnpoly.overall,
XGB = conf.mat.xgboost.overall) %>%
t() %>%
kbl(booktabs =  T, caption = "Model Performance") %>%
kable_styling(latex_options = c("striped, HOLD_position"), full_width = F) %>%
footnote(c(
"Variables from PCA & factor analysis considered in model performance evaluation"
))
