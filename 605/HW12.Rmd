---
title: "HW12"
author: "Zachary Palmore"
date: "4/20/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Directions

The attached who.csv dataset contains real-world data from 2008. The variables included follow.

Country: name of the country
LifeExp: average life expectancy for the country in years
InfantSurvival: proportion of those surviving to one year or more
Under5Survival: proportion of those surviving to five years or more
TBFree: proportion of the population without TB.
PropMD: proportion of the population who are MDs
PropRN: proportion of the population who are RNs
PersExp: mean personal expenditures on healthcare in US dollars at average exchange rate
GovtExp: mean government expenditures per capita on healthcare, US dollars at average exchange rate
TotExp: sum of personal and government expenditures.

    1. Provide a scatterplot of LifeExp~TotExp, and run simple linear regression. Do not transform the 
    variables. Provide and interpret the F statistics, R^2, standard error,and p-values only. Discuss 
    whether the assumptions of simple linear regression met.
    
    2. Raise life expectancy to the 4.6 power (i.e., LifeExp^4.6). Raise total expenditures to the 0.06 
    power (nearly a log transform, TotExp^.06). Plot LifeExp^4.6 as a function of TotExp^.06, and r 
    re-run the simple regression model using the transformed variables. Provide and interpret the F 
    statistics, R^2, standard error, and p-values. Which model is "better?"
    
    3. Using the results from 3, forecast life expectancy when TotExp^.06 =1.5. Then forecast life 
    expectancy when TotExp^.06=2.5. 
    
    4. Build the following multiple regression model and interpret the F Statistics, R^2, standard error, 
    and p-values. How good is the model?
    LifeExp = b0+b1 x PropMd + b2 x TotExp +b3 x PropMD x TotExp
    
    5. Forecast LifeExp when PropMD=.03 and TotExp = 14. Does this forecast seem realistic? Why 
    or why not?


## Part 1

First, the data is read and the first few rows are shown for reference. It should match our variables above. 

```{r}
data <- read.csv("https://raw.githubusercontent.com/palmorezm/msds/main/605/who.csv")
head(data)
```

Next we create a scatter plot with the sum of personal and government expenditures (or total expenditures) on the x-axis and average life expectancy for each country in years on the y-axis. 


```{r}
plot(data$TotExp, data$LifeExp)
```

Our main takeaway is that the points do not follow a straight liner trend. There is a clear pattern that might be exponential or logistic but nothing about the relationship between the two indicates a steady increase or decrease as one changes. Regardless, we run a linear model to assess a few statistics. In it we ensure that life expectancy (LifeExp) is modeled by the total expenditures (TotExp). The steps and summary are provided below. 

```{r}
lm.data <- lm(LifeExp~TotExp, data)
summary(lm.data)
```

This summary shows that the coefficient of determination, or $R^2$, is `r summary(lm.data)$r.squared` which describes how well the regression line explains the data. In this case, it specifically looks at how much variation in life expectancy is explained by the total personal and government expenditures. As expected, a linear trend on this data without transformation is poor, with the $R^2$ explaining only about 25% of the model fit. Many would consider this a weakly correlated value given that the range of possible correlation strength extends from 0 to 1 with one indicating a perfect fit for all points. 

The p-value for total expenditures is `r summary(lm.data)$coefficients[1,"Pr(>|t|)"]` which is statistically significant beyond the alpha level of 0.001. If our null hypothesis is that adding our variable would result in no change in our model, then we could reject the null. In this case, the addition of total expenditures significantly changed the estimates. However, statistical significance alone, does not mean we can make accurate predictions from it. This p-value only shows that the probability of this change occurring by random chance is extremely low. 

Results also show that the F statistic is `r summary(lm.data)$fstatistic["value"]`. The F statistic is in an indication of overall significance in the model. It tells us whether the regression plotted has a better fit than an intercept-only model. The further the value is from 1, the better the model. Our critical value is probably less than 2 with 188 degrees of freedom and this value is greater than that critical value. Thus, we could reject the null hypothesis to conclude that the addition of total expenditures is statistically significant and that there is a strong relationship between the variables (stronger than the null hypothesis). However, here again, significance does not equate to a perfect linear model nor its ability to be useful. 

Going further, the standard error is `r summary(lm.data)$coefficients[1,"Std. Error"]`. The closer this value is to 0, the better, since it indicates how far the data is from the model expectations on average. Notice that this value is extremely low. Some consider this an indicator of the quality of the fit in a model. This low value indicates high quality, assuming the other assumptions of linear regression are met. Additionally, the total expenditure coefficient is `r summary(lm.data)$coefficients[1,"Estimate"]` +/- `r summary(lm.data)$coefficients[1,"Std. Error"] * 1.96` at 95% confidence. 

Based on these statistics, the assumptions of simple linear regression are not met. Consider that the initial scatter plot displayed no signs of linearity, an important assumption in linear regression. For us to use linear regression, one must assume that there is a linear relationship in the data. In this case, there is not. Additionally, the normality of a plot such as the one shown, would also exhibit non-normal trends. These two failures in our assumptions would eliminate the option of using a linear regression model to make predictions or interpretations without transformations.  


## Part 2

To see if the data can appear more linear and normal it is transformed. The total expenditures values are raised to the power of 0.06 and the life expectancy values to the power of 4.6. The resultant scatterplot is shown below.  

```{r}
plot(data$TotExp^.06, data$LifeExp^4.6)
```

This shows a much more linear trend with greater potential to fulfill the assumptions of linear regression. We continue with a summary of the linear model to compare those statistics previously mentioned. 

```{r}
data$LifeExp4.6 <- data$LifeExp^4.6
data$TotExp.06 <- data$TotExp^.06
lm.data.t <- lm(LifeExp4.6~TotExp.06, data)
summary(lm.data.t)
```

This summary shows that the coefficient of determination, or $R^2$, is `r summary(lm.data.t)$r.squared` which is much better than our previous model. With this statistic one might assume that the variation in life expectancy explains about 72% of the expenditures. This is a much strong correlation than our original model. 

The p-value for total expenditures is `r summary(lm.data.t)$coefficients[1,"Pr(>|t|)"]` which is statistically significant beyond the alpha level of 0.001. Again, the null hypothesis assumes the addition of total expenditures is not significant and could be due to random chance. In this case, our probability of these changes occurring by chance is again, extremely low, but this p-value also happens to match that of the model without any independent variables. 

These results show that the F statistic is `r summary(lm.data.t)$fstatistic["value"]` suggesting that this model is almost certainly significantly and we can reject the null hypothesis. This and the p-value means that the addition of total expenditure is statistically significant in our model and that there is a closer relationship between the variables than in our previous model. However, there is something else that must be considered. 

The standard error of this transformed data set is `r summary(lm.data.t)$coefficients[1,"Std. Error"]`. Recall that, the closer this value is to 0, the better, since it indicates how far the data is from the model expectations on average. This error value is extremely high suggesting the data associated with this model are extremely far away from their fit in the model. As an indicator of model quality, this would be a very poor model. However, based on the information observed and how we were directed, this model is technically better because it allows us to satisfy the necessary assumptions for linear regression and create the model under more appropriate conditions. Although, it is practically useless for prediction due to its high error and these transformations. 



## Part 3

To forecast life expectancy based on two values of total expenditures, located at 1.5 and 2.5, we create a new data frame using those values and predict with it. Confidence intervals accompany each prediction which is based on the transformed model. 

```{r}
newdata <- data.frame(TotExp.06=c(1.5,2.5))
predict(lm.data.t, newdata, interval="predict")^(1/4.6)
```

Based on this output Life expectancy is about 63 years when total expenditures is at a value of about 1.5. Life expectancy then increases with increased expenditures to about 87 when the total expenditures value is at 2.5. The 95% confidence intervals are from approximately 36 - 73 years when total expenditures is at 1.5 and about 82 - 90 years when total expenditures is at 2.5. 




## Part 4

```{r}
lm.example <- lm(LifeExp ~ PropMD + TotExp + TotExp:PropMD, data)
summary(lm.example)
```

Comparing the same statistics as previously mentioned, this model is generally worse than the second model created with the transformations. To start, the $R^2$ value is much lower, indicating that the model does not explain most of the variation between life expectancy and total expenditures. The F-statistic is also several times smaller than our transformed model. Results are still significant but to a lesser degree. The standard error is  also lower, much closer to zero but that may not be enough. If we were to examine more we might create some diagnostic plots to validate the conditional assumptions of linearity as shown:

```{r}
par(mfrow=c(2,2))
plot(lm.example)
```

Looking at the diagnostic plots, there is a clear pattern in the residuals and normality is a little off. Thus, our assumptions are already invalid. However, there is also a problem with overly influential values and their leverage on the data, as well as residuals being unequally spread in the Scale-location plot. Nothing about this model indicates a good linear fit for this data. 

## Part 5

Our final prediction with the new metrics of a .3

```{r}
newdata.example <- data.frame(PropMD=0.03, TotExp=14)
predict(lm.example, newdata.example, interval="predict")
```


























































