---
title: "HW3"
subtitle: "Business Analytics and Data Mining"
author: "Zachary Palmore"
date: "3/26/2021"
output:
  rmdformats::html_clean:
    highlight: "monochrome"
    code_folding: "hide"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Assignment 3

___

## Purpose

In this homework assignment, we will explore, analyze and model a data set containing information on crime for various neighborhoods of a major city. Each record has a response variable indicating whether or not the crime rate is above the median crime rate (1) or not (0). 

Our purpose is to build a binary logistic regression model on the training data set to predict whether the neighborhood will be at risk for high crime levels. We will provide classifications and probabilities for the evaluation data set using our binary logistic regression model. We can only use the variables given to us (or variables that are derived from the variables provided).


## Introduction 

```{r message=FALSE}
# Packages
library(tidyverse)
library(reshape2)
library(ggpubr)
library(ggcorrplot)
library(kableExtra)
library(Amelia)
library(caret)
library(pROC)
library(psych)
library(bestNormalize)
theme_set(theme_minimal())
# Data
cdata <- read.csv("https://raw.githubusercontent.com/palmorezm/msds/main/621/HW3/crime-training-data_modified.csv")
cdata.eval <- read.csv("https://raw.githubusercontent.com/palmorezm/msds/main/621/HW3/crime-evaluation-data_modified.csv")
```

There are 466 observations in this data set with 13 different variables. Each observation is a statistical summary that indicates an attribute associated with a particular neighborhood within the major city. For example the first variable 'zn' contains the proportion of residential land zoned for lots that are over 25000 square feet. The first five observations of each variable are shown. 

```{r}
first5.tbl <- cdata[1:5,]
first5.tbl 
```

Our target variable is labeled 'target' and it describes whether the neighborhood's crime rate is above the median or below it. Any or all of these variables could be used as our predictors except the dummy variable of 'chas' and our target variable. In the next section we will explore the relationships between these predictors and review their interaction with our target to see which of the variables make the best predictors of crime rate. A full list of variable descriptions is also provided for reference. 

```{r}
vardesc <- data.frame(matrix(c(
'zn',	'proportion of residential land zoned for large lots (over 25000 square feet)',
'indus',	'proportion of non-retail business acres per suburb',
'chas',	'a dummy var. for whether the suburb borders the Charles River (1) or not (0)',
'nox',	'nitrogen oxides concentration (parts per 10 million)',
'rm',	'average number of rooms per dwelling',
'age',	'proportion of owner-occupied units built prior to 1940',
'dis',	'weighted mean of distances to five Boston employment centers',
'rad', 'index of accessibility to radial highways',
'tax',	'full-value property-tax rate per $10,000',
'ptratio',	'pupil-teacher ratio by town',
'lstat',	'lower status of the population (percent)',
'medv',	'median value of owner-occupied homes in $1000s',
'target',	'whether the crime rate is above the median crime rate (1) or not (0)'), byrow = TRUE, ncol = 2))
colnames(vardesc) <- c('Variable', 'Description')
vardesc
```

## Data Exploration

To determine which of the variables might make the best predictors we must first explore the data for potential sources of error. This includes, identifying and fixing missing values (if there are any), calculating and comparing averages, standard deviations, and other summary statistics. We begin this by checking to see if any of the data are missing. Instead of relying solely on numbers, we created a map of observations of each variable. Blue indicates the presence of an observation and light grey indicates an absence.

```{r}
cdata.missingobservatons <- sum(is.na(cdata))
missmap(cdata, main = "Missing and Complete Observations")
```

This map appears to be a solid color of blue which indicates that none of the data are missing. We can also confirm this statistically where our calculations of any non-applicable value would be counted and tallied then take the sum of those tallies for a total value missing. Perhaps as expected, the total missing from statistical calculations is `r cdata.missingobservations`. This is wonderful news since it means we will not need to impute but exceedingly abnormal given this kind of model.

Next we calculate and compare the averages, standard deviations, and other summary statistics. These will be used to moderate our expectations of the variables in our model and inform us of the best ways to prepare the data if there are any changes that need to be made. Those statistics are shown below. 

```{r}
summary.tbl <- describe(cdata)
summary.tbl[1:13,c(2:5, 8:10, 13)]
```

Since it is quite rare to have a fully filled data set, we further confirm the presence of each observation with the 'n' column. This column describes how many observations there are of each variable. Here again, we are reassured that this data is fairly clean already with the same number of observations for each variable. 

To get a sense of scale, magnitude, and direction for these vectors we calculated the mean, median, and range for each variables. Notice that the potential predictors of 'tax' and 'age' seem to be on a different scale than the remaining variables. There is also a relatively large standard error for the the variables 'zn,' 'age,' and 'tax' when compared with the other potential predictors. Many of these variables have small deviations from their means or low variance with standard deviations less than 10. However, those same three predictors mentioned, do not. 

We recognize that any of these variables could confound existing relationships with our target; or may turn out to be complete outliers that could skew our model. But those three, 'zn,' 'age,' and 'tax' are enough cause for concern that we need a deeper look at all predictors. We should make every attempt to avoid spurious associations if we are to build an accurate model. 

Before delving into the nuances of these variables' distributions and spreads, we take a random sample of the complete table of crime data and divide it into training and evaluation data sets using a 70-30 split. Given that we had 466 observations within the complete crime data list we should expect about 327 observations in the resultant training data with the remaining 139 observations in the evaluation set. 

After, we recalculate the same summary statistics on our training set called 'train.' Ideally, these identical calculations will produce values that preserve the essence of the orignial data. This helps ensure we make the best model for real-world scenarios. This process is documented below. 

```{r}
set.seed(72747)
indecies <- createDataPartition(cdata$target, p = .7, list = FALSE, times = 1)
train <- cdata[indecies,]
eval <- cdata[-indecies,]
train.summary.tbl <- train %>% 
  select(-target) %>% 
  describe()
train.summary.tbl[1:12,c(2:5, 8:10, 13)]
```

Results indicate that our random sample split worked properly with 327 observations of all potential predictors in the training set. Our calculated values also remain close to the original with the most variation occurring in those same three previously mentioned variables 'zn,' 'age,', and 'tax.' Thus, the essence of the crime data tables has been persevered. These predictors will likely need adjusting but for now, we continue exploring the data. 

Now, we check for patterns in the distributions of the predictors. Finding those that might not fit within our model improves our precision. We can also being to check for conditional assumptions that must be made for us to be able perform a logistic regression. Keep a close look on how these variables differ in the histogram-frequency chart below. 

```{r}
train[1:12] %>%   
  melt() %>% 
  ggplot() +                      
    geom_histogram(aes(value, alpha = 2, fill = variable)) +
    facet_wrap(~ variable, scales = "free") + 
    geom_freqpoly(aes(value), bins=15, lty = 3) +
    labs(title = "Distribution of Predictors", subtitle = "With Variable Frequency Overlay") +
    theme(axis.title = element_blank(), 
          legend.position = "none", 
          plot.title = element_text(hjust = 0.5), 
          plot.title.position = "panel",
          plot.subtitle = element_text(hjust = 0.5)) 
```

One method to evaluate distribution is using the kernel density estimates of values present beneath each predictor. In doing so we can create another visual aid to show how the density of predictors changes over its respective distribution. This is shown below. 

```{r}
train[1:12] %>%   
  melt() %>% 
  ggplot() +                      
    geom_density(aes(value, alpha=.50, fill = variable)) +
    facet_wrap(~ variable, scales = "free") +
    labs(title = "Predictor Density", 
         subtitle = "Using Kernal Density Estimations (KDE)") +
    theme(axis.title = element_blank(), 
          legend.position = "none", 
          plot.title = element_text(hjust = 0.5), 
          plot.title.position = "panel",
          plot.subtitle = element_text(hjust = 0.5)) 
```

It appears some predictors are clearly being overly influenced by outliers. Meanwhile, the 'chas' distribution is binary like our target and offers no real benefit to our model. Addtionally, some predictors exhibit normality such as 'rm' and 'medv.' Unfortunately normality is not a requirement of logistic regression. Although it is an immensely useful tool to understand how the model is expected to function. In order to further improve real-world accuracy, we should remind ourselves of what we need to validate before building a model. 

The conditions of binary logistic regression include: independence of observations, a dichotomous dependent variable, little to no multicollinearity, linearity of the logit odds, and a large enough sample size relative to the expected probability of the least frequent outcome. These are evaluated in several ways, the first of which we can review with a kernel density estimation with predictors plotted by color. Since there are twelve predictors, it may appear a little messy but remember we are observing overall trends. Labels will not be necessary since the name, type, or association of color with predictor is irrelevant.   

```{r include=F, eval=F}
# Overall predictor density 
train %>% 
  melt() %>% 
  ggplot() + 
  geom_density(aes(value))
# Transposed predictor spread
  t.train <- data.frame(t(train))
  t.train %>%
    melt() %>% 
    ggplot() +
    geom_point(aes(value, variable, color = variable)) + 
    theme(legend.position = "none",
          axis.title.x = element_blank(), 
          axis.text.y = element_blank()) 
```



```{r}
t.train <- data.frame(t(train))
m.train <- train %>% 
  melt()
f.train <- cbind(m.train, t.train)

# Overal kernal density estimation
kde.lim125 <- f.train %>% 
  rename("predictor" = variable, 
         "m.vals" = value) %>% 
  melt()  %>% 
  ggplot() + 
  geom_density(aes(value, color = variable, alpha=.1)) + 
  labs(title = "Predictor KDE Cluster 1") +
  theme(legend.position = "none",
          axis.title.x = element_blank(), 
          axis.text.y = element_blank()) +
  geom_blank(aes(value), binwidth = 5) + 
  geom_density(aes(value, color = variable, alpha=.1)) + 
  theme(legend.position = "none",
          axis.title.x = element_blank(), 
          axis.text.y = element_blank()) + 
  xlim(0, 125)
kde.nolim <- f.train %>% 
  rename("predictor" = variable, 
         "m.vals" = value) %>% 
  melt()  %>% 
  ggplot() + 
  geom_density(aes(value, color = variable, alpha=.1)) + 
  labs(title = "Englarged Predictor KDE") + 
  theme(legend.position = "none",
          axis.title.x = element_blank(), 
          axis.text.y = element_blank()) +
  geom_blank(aes(value), binwidth = 5) 
ggarrange(kde.nolim, kde.lim125)
```

From this we can infer several things. First that the data is clustered into three to four major groups, in which almost all of the data is concentrated into the first group. Second, the first group contains data that appear concentrated at 0 and 1 which, in a continuous distribution of density, is not beneficial. These variables should be removed in data preparation. 

This graphic also implies there are outliers that contribute to prediction by asymmetrical design and thus increased error in our model. These too should be dealt with in data preparation. Given the variation in peaks of cluster 1, it is likely that our observations are independent of one another although we cannot confirm this assumption from the graphic alone. 

```{r include=F, eval=F}
pnt.lim125 <- f.train %>% 
  rename("predictor" = variable, 
         "m.vals" = value) %>% 
  melt()  %>% 
  ggplot() + 
  geom_point(aes(value, variable, color = variable, alpha=.1)) + 
  xlim(0,125) +
  labs(title = "Predictor KDE Cluster 1") +
  theme(legend.position = "none",
          axis.title.x = element_blank(), 
          axis.text.y = element_blank())
pnt.nolim <- f.train %>% 
  rename("predictor" = variable, 
         "m.vals" = value) %>% 
  melt()  %>% 
  ggplot() + 
  geom_point(aes(value, variable, color = variable, alpha=.1)) + 
  labs(title = "Englarged Predictor KDE") + 
  theme(legend.position = "none",
          axis.title.x = element_blank(), 
          axis.text.y = element_blank()) +
  geom_blank(aes(value), binwidth = 5) 
ggarrange(pnt.nolim, pnt.lim125)
```



```{r include=F, eval=F}
f.train %>% 
  rename("predictor" = variable, 
         "m.vals" = value) %>% 
  melt()  %>% 
  group_by(predictor) %>%
  mutate(predictor.med = median(value), 
            predictor.mean = mean(value)) %>%
  ggplot() + 
  geom_point(aes(predictor.med, predictor, color = predictor)) 

f.train %>% 
  rename("predictor" = variable, 
         "m.vals" = value) %>% 
  melt()  %>% 
  group_by(predictor) %>%
  mutate(predictor.med = median(value), 
            predictor.mean = mean(value)) %>%
  ggplot() + 
  geom_point(aes(predictor.mean, predictor, color = predictor)) 
```



```{r}
train[1:12] %>% 
  melt() %>% 
  ggplot(, aes(variable, value)) + 
  geom_violin(aes(variable, value, fill=variable)) + 
  geom_boxplot(aes(variable, value, color = variable)) + 
  guides(color=FALSE, size=FALSE) +
  ggtitle("Predictor Boxplots") +
  theme(axis.title = element_blank()) 
```

Based on the predictor boxplot above and our previous research, we can break the data into groups. In this boxplot, also note just how abnormal the predictors of 'tax' and 'age' are over the entire model. At this point, we could confidently separate them to avoid making error-prone estimates of the crime rate. This forms one group. We repeat this process over the remaining predictors until they fit with at least one other characteristically similar predictor that is based on scale, IQR, median, and outliers. 

```{r}
# seperate into groups 
zimlp <- train[,c("zn", "indus", "medv", "lstat", "ptratio")]
ta <- train[,c("tax", "age")]
rdr <- train[,c("rad", "dis", "rm")]
nc <- train[,c("nox", "chas")]
```

With these predictor groups, we can create violin plots with those boxplots to describe the median and outliers of each predictor. It is another, more useful way to visualize the distribution while evaluating those conditional assumptions of logistic regression to guarantee model adheres to the rules of binary logistic modeling. Kernel density estimates are placed on here as the black lines that create the 'violin' shapes to gauge where in the distribution point are concentrated. This is shown relative to the predictor's own mean, IQR, range, deviations, and shows variation across the spread of other predictors in their clustered groups.   

```{r}
viobox.zimlp <- zimlp %>% 
  melt() %>% 
  ggplot() + 
  geom_violin(aes(variable, value, fill=variable, fatten=2)) +
  geom_boxplot(aes(variable, value, color = variable, alpha = .90)) +
  xlab("Value") + ylab("Variable") + 
  theme(legend.position = "none", axis.title = element_blank()) +
  coord_flip()
viobox.ta <- ta %>% 
  melt() %>% 
  ggplot() + 
  geom_violin(aes(variable, value, fill=variable, fatten=2)) +
  geom_boxplot(aes(variable, value, color = variable, alpha = .90)) +
  xlab("Value") + ylab("Variable") + 
  theme(legend.position = "none", axis.title = element_blank()) +
  coord_flip()
viobox.rdr <- rdr %>% 
  melt() %>% 
  ggplot() + 
  geom_violin(aes(variable, value, fill=variable, fatten=2)) +
  geom_boxplot(aes(variable, value, color = variable, alpha = .90)) +
  xlab("Value") + ylab("Variable") + labs(title = "Predictor Summary Plots",
                                          subtitle = "Using Mean, Density, & Spread") + 
  theme(legend.position = "none", axis.title = element_blank()) +
  coord_flip()
viobox.nc <- nc %>% 
  melt() %>% 
  ggplot() + 
  geom_violin(aes(variable, value, fill=variable, fatten=2)) +
  geom_boxplot(aes(variable, value, color = variable, alpha = .90)) +
  xlab("Value") + ylab("Variable") + 
  theme(legend.position = "none", axis.title = element_blank()) +
  coord_flip()
ggarrange(viobox.rdr, viobox.zimlp, viobox.nc, viobox.ta) 
```

The predictors 'rm,' 'ptratio,' 'medv,' 'lstat,' and 'nox,' are the most likely choices to make an inference on at this moment. This is because they are suited to predict with; that is, they have an average near the center of their distribution, are more unimodal than other options, and contain a spread with minimal outliers. While this makes it easy to see which distributions' properties it is hard to discern exactly what the magnitude of influence outliers could play in each predictors' distribution. For that, we repeat the process with a new plot. 


```{r}
# Function to calculate and set pointrange
xysdu <- function(x) {
   m <- mean(x)
   ymin <- m - sd(x)
   ymax <- m + sd(x)
   return(c(y = m, ymin = ymin, ymax = ymax))
}

colors <- c("Median" = "Red", "Mean" = "Black")
ptrng.zimlp <- zimlp %>% 
  melt() %>% 
  ggplot(aes(variable, value)) + coord_flip() + 
  stat_summary(fun.data=xysdu, geom = "Pointrange", shape=16, size=.5, color="black") +
  stat_summary(fun.y=median, geom="point", shape=16, size=2, color="red") + 
  theme(legend.position = "None", axis.title.x = element_blank(), axis.title.y = element_blank())
ptrng.rdr <- rdr %>% 
  melt() %>% 
  ggplot(aes(variable, value)) + coord_flip() + 
  stat_summary(fun.data=xysdu, geom = "Pointrange", shape=16, size=.5, color="black") +
  stat_summary(fun.y=median, geom="point", shape=16, size=2, color="red") + 
  labs(title = "Characteristic Outlier Analysis", 
       subtitle = "By Preditor Mean & Median") + 
  scale_color_manual(values = colors) +
  theme(legend.position = "Bottom", axis.title.x = element_blank(), axis.title.y = element_blank())
ptrng.ta <- ta %>% 
  melt() %>% 
  ggplot(aes(variable, value)) + coord_flip() + 
  stat_summary(fun.data=xysdu, geom = "Pointrange", shape=16, size=.5, color="black") +
  stat_summary(fun.y=median, geom="point", shape=16, size=2, color="red") + 
  theme(legend.position = "None", axis.title.x = element_blank(), axis.title.y = element_blank())
ptrng.nc <- nc %>% 
  melt() %>% 
  ggplot(aes(variable, value)) + coord_flip() + 
  stat_summary(fun.data=xysdu, geom = "Pointrange", shape=16, size=.5, color="black") +
  stat_summary(fun.y=median, geom="point", shape=16, size=2, color="red") + 
  theme(legend.position = "None", axis.title.x = element_blank(), axis.title.y = element_blank())
ggarrange(ptrng.rdr, ptrng.zimlp, ptrng.nc, ptrng.ta) 
```


Taking advantage of a statistical phenomena, we find the pointrange of each predictor by exploiting the difference in robustness of mean and median and graphing them on top of the respective predictor ranges. Recall these predictors have already been grouped by similar values as shown in the violin plots above. These predictors are situated in the same groups and positioning for quick referencing. 

When evaluating in this way, the predictorsâ€™ medians are plotted as red dots on a black line that is the point range calculated from the standard deviation of each distribution. In the center of that point range is a black dot that represents the mean of each predictor. The farther away the red and black dots are from one another, the more influence outliers have on the predictor.

This pattern emphasizes the difference within clusters to eliminate the potential for conflicts of size and scale. On a scale of similar values, we can tell just how much influence each predictor harbors compared to others in its group. In the upper left quadrant, the predictor 'rad' has the most skewness. In the upper right quadrant, 'zn.' The best predictors shown seem to be 'nox,' 'pratio,' 'lstat,' and 'rm,' with potential from 'medv,' 'age,' and 'dis.'. Although there is one more assumption we should consider, multicollinearity.  


```{r}
train %>% 
  cor() %>% 
  ggcorrplot(method = "square", type="upper", ggtheme = ggplot2::theme_minimal, legend.title = "Influence") + coord_flip()
```

There is a near perfect correlation in the positive direction between the predictors 'rad' and 'tax.' To avoid entanglement, these cannot be modeled simultaneously. A few other predictors, such as 'age' and 'dis' are closely related, but not close enough to exclude from the analysis. 

```{r}
predictors <- (length(colnames(train))-1)
Pexpected <- 1 # If we expect at least one neighborhood above median
case.min <- 10 # from general rule and practice
est.samplesize <- (case.min*predictors)/Pexpected
```


To review, the dependent variable is binary which makes it dichotomous. So this condition is satisfied. We also assume based on the evidence we have, including that these continuous variables are proportions of the whole for each neighborhood, that observations made are independent of one another. Linearity of log odds will be evaluated once the data is prepared. We could compute if the sample size is large enough through the total predictor variables and expected probability of the least frequent outcome, but the information in this data set comes from summarized proportions and averages (medians). For this reason, our best estimate of an accurate sample size for the study is about `r est.samplesize` if we also assume that the expected probability of our least frequent outcome is one. 



## Data Preparation

To prepare the data we solve the potential problems found during exploration. They were to adjust scales to ensure similar comparisons. To remove outliers and reduce the influence exerted by extreme values to maintain satisfaction with logistical modeling's conditional statements. To prevent autocorrelation and transform the data to produce a Gaussian training set that allows users to visually interpret predictions with ease. Recall, there were no missing values and thus no imputation is required. 


```{r}
# exclude nonmeaningful discrete ints, target, chas in reduction process
df <- train %>% 
  select(-c(target, chas))
# remove outliers based on IQR
for (i in colnames(df)) {
  iqr <- IQR(df[[i]])
  q <- quantile(df[[i]], probs = c(0.25, 0.75), na.rm = FALSE)
  qupper <- q[2]+1.5*iqr
  qlower <- q[1]+1.5*iqr
  outlier_free <- subset(df, df[[i]] > (q[1] - 1.5*iqr) & df[[i]] < (q[2]+1.5*iqr) )
}
df <- outlier_free
```


As shown above, outliers were removed using the interquartile ranges of each predictor. We also excluded discrete integers and the target since they would not contribute meaningfully to the analysis. This reduced the dimensions of our data frame to 304 observations of 11 predictors.

In this reduction process, and where applicable, if values extended too far beyond their upper or lower ranges, they were removed preserving only those values that might be considered normal for median value or proportions of the attribute in a neighborhood. This is determined mathematically by 2 quantiles above or below the upper or lower quartiles respectively.

We continue to prepare the data with an evaluation of transformation methods. Although normalized data is not essential for categorical analysis such as this one, it is valuable for visual cues which helps us test and improve the model. In this case, we loop through each variable to determine the best method for a Gaussian display. 


```{r}
bestNorms <- df[1:11,]
for (i in colnames(df)) {
  bestNorms[[i]] <- bestNormalize(df[[i]], 
                                  allow_orderNorm = FALSE, 
                                  out_of_sample =FALSE)
}
```



```{r}
bestNorms$zn$chosen_transform
bestNorms$indus$chosen_transform
bestNorms$nox$chosen_transform
bestNorms$rm$chosen_transform
bestNorms$age$chosen_transform
bestNorms$dis$chosen_transform
bestNorms$rad$chosen_transform
bestNorms$tax$chosen_transform
bestNorms$ptratio$chosen_transform
bestNorms$lstat$chosen_transform
bestNorms$medv$chosen_transform
```

Based on the recommendations of the bestNormalize function, there are two main transformations, Box-Cox and log_x. Rather than perform their best respective transformations on each predictor, we will only perform them on three specific variables that we suspect will predict better than most others. The process is documented in the creation of our finalized data set called 'training.' 

```{r}
# focus on specific predictors 
bxcx.medv <- boxcox(df$medv)
bxcx.ptratio <- boxcox(df$ptratio)
logx.lstat <- log_x(df$lstat)
transformed <- data.frame(logx.lstat$x, bxcx.medv$x, bxcx.ptratio$x)
transformed <- transformed %>% 
  rename("lstat2"=logx.lstat.x,
         "medv2"=bxcx.medv.x,
         "ptratio2"=bxcx.ptratio.x)
evalu <- eval %>% 
  select(-"chas")
zip <- cbind(df, transformed)
zip$target <- sample(train$target, 304, replace = FALSE)
zip <- zip[,c(1:8, 12:15)] %>% 
  rename("lstat"=lstat2,
         "medv"=medv2,
         "ptratio"=ptratio2)
training <- zip
```

Our first model will be one based on a single predictor to build on. It is included in the preparation section to test our prepared training data and to inform us of where our model should improve when we begin building. Of course, this will include our target variable from the 'training' data set and be modeled by the 'medv' predictor; which is equivalent to median value of owner-occupied homes. 

This idea considers the hypothesis that wealthier homeowners are able to buy their way out of higher crime neighborhoods or into lower crime areas. This could be through the selling of higher valued homes or alternatively excluding those who cannot afford homes that might be more likely to commit a crime. We test this here in our first model, 'mod1'.

Generally speaking, when we consider criminal activity and the rate at which it occurs, we are not speaking of fraud, money laundering, or other high crimes and misdemeanors that typically are not associated with any specific location. We are only speaking of physical, reportable crimes that often have a discrete statistic with a specific victim or victims. Meanwhile, criminal activity with an broad, non-discrete victim and no physical location remain unreported. It is not something we can adjust for in this analysis, but it is worth discussing. 

```{r}
mod1 <- glm(target ~ medv, family = binomial(link = "logit"), training)
summary(mod1)
```

Our accuracy, precision, and other evaluation metrics can be calculated with a confusion matrix. We create this matrix by comparing our predicted values to the evaluation data set captured in the introduction. Importantly, we devised a function to compute F1 scores which will serve as the most useful guiding statistic in prediction evaluation. Results of this first model are shown below:

```{r}
pred.1 <- ifelse(predict.glm(mod1, evalu,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.1), 
                      factor(evalu$target),"1")
results <- tibble(model = "Model 1",
                  predictors = 1,
                  F1 = cm$byClass[7],
                  deviance=mod1$deviance,
                  r2 = 1 - mod1$deviance/mod1$null.deviance,
                  aic=mod1$aic)
cm
```

```{r}
F1 <- function(c) {
  sensitivity <- c$byClass[[1]]
  precision <- c$byClass[[5]]
  return( (2*precision*sensitivity) / 
            (precision + sensitivity))
}
F1.mod1 <- F1(cm)
```

This model's F1 score is `r F1.mod1` with an overall accuracy of about 0.46. This was expected. With the only predictor being medv and the transformation and reduction of this variable, this result is intentionally low. The goal is to be able to improve and this does exactly that. Now that we have the data prepared and the process tested, we can begin building other models to beat our prepared single predictor model. 


## Modeling Building

When building these it is imparative that we test the data so that we can comprehend how the model functioned with its features. With this analysis, we will place a focus on the F1 score calculation in testing for later evaluation during model selection. We begin by attempting to beat the scores of our prepared model. In this case, it means trying to include the most useful predictors that may increase our accuracy. This should not be too difficult since it was our intent to start at a low value. The same process documented in our data preparation model is utilized. 

```{r}
mod1.all <- glm(target ~ ., family = binomial(link = "logit"), training)
summary(mod1.all)
```

```{r}
pred.2 <- ifelse(predict.glm(mod1.all, evalu,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.2), 
                      factor(evalu$target),"1")
results <- rbind(results, tibble(model = "Model 1 All",
                  predictors = 11,
                  F1 = cm$byClass[7],
                  deviance=mod1.all$deviance,
                  r2 = 1 - mod1.all$deviance/mod1.all$null.deviance,
                  aic=mod1.all$aic))
cm
```

```{r}
F1.mod1.all <- F1(cm)
```

Including all predictor variables improved the model performance moderately from the first model maintaining our accuracy and raising the model's F1 score to `r F1.mod1.all`. However, this is still far from useful at prediction. At the moment we're not much better than random chance. To solve this, we review how the model would perform with minor alteration. We will call this 'mod2' for our second model using the second training data frame. Here again, we repeat the process with the same singular predictor selected previously.




```{r}
mod2 <- glm(target ~ medv, family = binomial(link = "logit"), train)
summary(mod2)
```

```{r}
pred.3 <- ifelse(predict.glm(mod2, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.3), 
                      factor(eval$target),"1")
results <- rbind(results, tibble(model = "Model 2",
                  predictors = 1,
                  F1 = cm$byClass[7],
                  deviance=mod2$deviance,
                  r2 = 1 - mod2$deviance/mod2$null.deviance,
                  aic=mod2$aic))
cm
```

```{r}
F1.mod2 <- F1(cm)
```

This model with minor changes and only a single predictor performs better than the full scale model with all predictors of the training set. We had an accuracy of approximately 0.69 and and F1 score of `r F1.mod2`. This is an improvement and so we move forward with this prepared data. Next we include all predictors for model 2. Recall that, this data set includes everything from the original crime data set which should give a better picture of what is happening with crime rate. 

```{r}
mod2.all <- glm(target ~ ., family = binomial(link = "logit"), train)
summary(mod2.all)
```

```{r}
pred.4 <- ifelse(predict.glm(mod2.all, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.4), 
                      factor(eval$target),"1")
results <- rbind(results, tibble(model = "Model 2 All",
                  predictors = 12,
                  F1 = cm$byClass[7],
                  deviance=mod2.all$deviance,
                  r2 = 1 - mod2.all$deviance/mod2.all$null.deviance,
                  aic=mod2.all$aic))
cm
```

```{r}
F1.mod2.all <- F1(cm)
```

This is our best model yet. Our accuracy has shot upwards to about 0.94 with an F1 score of `r F1.mod2.all`. However, there are still a few unexpected hickups. Several of these predictors are not significant predictors of our target crime rate. This combination of predictors also has enough false positive errors that we may be able to decrease our number of predictors to form another exquisite model with higher accuracy and precision yet again. To identify which predictors should be taken out of our final model we employ a backwards and forwards selection process to impliment only those predictors that improve our score when added. This results in our third model, 'mod3' by pulling the best parts of aforementioned models while eliminating the worst in an attempt to make a better prediction. 


```{r}
# eliminate variables as they improve accuracy and F1 score
mod3 <- glm(target ~ . -chas -rm, family = binomial(link = "logit"), train)
summary(mod3)

pred.5 <- ifelse(predict.glm(mod3, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.5), 
                      factor(eval$target),"1")
results <- rbind(results, tibble(model = "Model 3",
                  predictors = 9,
                  F1 = cm$byClass[7],
                  deviance=mod3$deviance,
                  r2 = 1 - mod3$deviance/mod3$null.deviance,
                  aic=mod3$aic))
cm
F1.mod3 <- F1(cm)
```


Through this forward selection of predictors based on insignificance, backward and forward selection of the remaining signifcant values with a p-value greater than 0.1 we determined that the only predictors to control were 'chas' and 'rm'. Excluding 'chas' marginally increased model accuracy and F1 score while the exclusion of 'rm' exhibited no changes. For this reason the predictor 'rm' was ultimately exlcuded because it did not add value to the model. Our final accuracy measurement was 0.9424 with an F1 score of `r F1.mod3`. 



## Select Models

Our criteria for selecting the best binary logistic regression model relies on predominantly on model accuracy, especially through its F1 score. This is based on the theory that in the real-world application of these models, the amount of false positives and false negatives in our prediction are critical for analysis. The F score also combines information from its formula on model sensitivity and precision. We also consider the number of predictors, with a preference for a lower number. Thinking in terms of applications of data collection, it might be more cost-effective if we did not need to collect as many predictors but were able to make the same predictions with minimal disturbance of our accuracy. As added measures, the coefficient of determination, deviance, and Akaike information criterion (AIC) are computed for each model. Should existing accuracy and predictors lead to narrow selection margin, a higher prediction accuracy in the form of a coefficient of determination, lower deviance, and better quality model as determined by AIC will be the ultimate decider. We arrange these statistics into a table:

```{r}
results
```

Model 2 with all predictors and Model 3 contain the best statistics. Final selection will be between those two. Keep in mind, the first  model, 'mod1' was intentionally made to produce poor results. If for some reason it had turned out to be one of the best predictors then we would know something was wrong. Thankfully, this is not the case. With slightly better F1 score, lower AIC, and lower deviance as well as fewer predictors required to make the prediction than the rest of the models, Model 3, appears to be the best selection possible. However, we will evaluate through their sensitivity and specificity in a receiver operating characteristic (ROC) plot with area under the curve (AUC) displayed for these two models.   

```{r}
par(mfrow=c(1,2))
pROC::plot.roc(eval$target, pred.5, print.auc=TRUE, type="s", main="Model 3")
pROC::plot.roc(eval$target, pred.4, print.auc=TRUE, type="s", main="Model 2 All")
```

As anticipated, the Model 3 performed better in this ROC plot and has a slightly better AUC. Due to its very improved sensitivity this model contains the best performance numbers for us to go with. In addition, it uses fewer predictors to reach that level of accuracy, sensitivity, precision, and specificity. This model is the best of the available models we could select from in this analysis. 



## Conclusion

Due to its improved performance statistics and use of fewer predictors to achieve the same or better levels of accuracy than other models, Model 3 is the best choice for making predictions with in a real-world setting. This model has minimal changes made to it, with minor adjustments in outlier quantity a subsequent reduction of overall observations and focusing of predictors on scale by clustering. If this analysis were to go further, it might be useful to identify other model types that could improve prediction accuracy while decreasing the predictors needed to make correct predictions. This might include an ensemble model that utilizes new predictors formed from the proportions of original predictors. However, for our purposes, an F1 score of `r F1.mod3` and accuracy of about 94.4% using only 9 raw predictors, will suffice for practical predictions. 
