mod2 <- glm(target ~ medv, family = binomial(link = "logit"), train)
summary(mod2)
pred.3 <- ifelse(predict.glm(mod2, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.3),
factor(eval$target),"1")
results <- tibble(model = "Model 2",
predictors = 1,
F1 = cm$byClass[7],
deviance=mod1$deviance,
r2 = 1 - mod1$deviance/mod1$null.deviance,
aic=mod1$aic)
cm
F1.mod2 <- F1(cm)
mod2.all <- glm(target ~ ., family = binomial(link = "logit"), train)
summary(mod2.all)
pred.4 <- ifelse(predict.glm(mod2.all, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.4),
factor(eval$target),"1")
results <- tibble(model = "Model 2 All",
predictors = 1,
F1 = cm$byClass[7],
deviance=mod1$deviance,
r2 = 1 - mod1$deviance/mod1$null.deviance,
aic=mod1$aic)
cm
F1.mod2.all <- F1(cm)
# eliminate variables as they improve accuracy and F1 score
mod3 <- glm(target ~ . -chas, family = binomial(link = "logit"), train)
# summary(mod3)
pred.5 <- ifelse(predict.glm(mod3, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.5),
factor(eval$target),"1")
results <- tibble(model = "Model 3",
predictors = 1,
F1 = cm$byClass[7],
deviance=mod1$deviance,
r2 = 1 - mod1$deviance/mod1$null.deviance,
aic=mod1$aic)
cm
F1(cm)
# eliminate variables as they improve accuracy and F1 score
mod3 <- glm(target ~ . -chas -zn, family = binomial(link = "logit"), train)
# summary(mod3)
pred.5 <- ifelse(predict.glm(mod3, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.5),
factor(eval$target),"1")
results <- tibble(model = "Model 3",
predictors = 1,
F1 = cm$byClass[7],
deviance=mod1$deviance,
r2 = 1 - mod1$deviance/mod1$null.deviance,
aic=mod1$aic)
cm
F1(cm)
# eliminate variables as they improve accuracy and F1 score
mod3 <- glm(target ~ . -chas -indus, family = binomial(link = "logit"), train)
# summary(mod3)
pred.5 <- ifelse(predict.glm(mod3, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.5),
factor(eval$target),"1")
results <- tibble(model = "Model 3",
predictors = 1,
F1 = cm$byClass[7],
deviance=mod1$deviance,
r2 = 1 - mod1$deviance/mod1$null.deviance,
aic=mod1$aic)
cm
F1(cm)
# eliminate variables as they improve accuracy and F1 score
mod3 <- glm(target ~ . -chas -rm, family = binomial(link = "logit"), train)
# summary(mod3)
pred.5 <- ifelse(predict.glm(mod3, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.5),
factor(eval$target),"1")
results <- tibble(model = "Model 3",
predictors = 1,
F1 = cm$byClass[7],
deviance=mod1$deviance,
r2 = 1 - mod1$deviance/mod1$null.deviance,
aic=mod1$aic)
cm
F1(cm)
# eliminate variables as they improve accuracy and F1 score
mod3 <- glm(target ~ . -chas, family = binomial(link = "logit"), train)
# summary(mod3)
pred.5 <- ifelse(predict.glm(mod3, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.5),
factor(eval$target),"1")
results <- tibble(model = "Model 3",
predictors = 1,
F1 = cm$byClass[7],
deviance=mod1$deviance,
r2 = 1 - mod1$deviance/mod1$null.deviance,
aic=mod1$aic)
cm
F1(cm)
# eliminate variables as they improve accuracy and F1 score
mod3 <- glm(target ~ . -chas -rm, family = binomial(link = "logit"), train)
# summary(mod3)
pred.5 <- ifelse(predict.glm(mod3, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.5),
factor(eval$target),"1")
results <- tibble(model = "Model 3",
predictors = 1,
F1 = cm$byClass[7],
deviance=mod1$deviance,
r2 = 1 - mod1$deviance/mod1$null.deviance,
aic=mod1$aic)
cm
F1(cm)
# eliminate variables as they improve accuracy and F1 score
mod3 <- glm(target ~ . -chas -rm -tax, family = binomial(link = "logit"), train)
# summary(mod3)
pred.5 <- ifelse(predict.glm(mod3, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.5),
factor(eval$target),"1")
results <- tibble(model = "Model 3",
predictors = 1,
F1 = cm$byClass[7],
deviance=mod1$deviance,
r2 = 1 - mod1$deviance/mod1$null.deviance,
aic=mod1$aic)
cm
F1(cm)
# eliminate variables as they improve accuracy and F1 score
mod3 <- glm(target ~ . -chas -rm -lstat, family = binomial(link = "logit"), train)
# summary(mod3)
pred.5 <- ifelse(predict.glm(mod3, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.5),
factor(eval$target),"1")
results <- tibble(model = "Model 3",
predictors = 1,
F1 = cm$byClass[7],
deviance=mod1$deviance,
r2 = 1 - mod1$deviance/mod1$null.deviance,
aic=mod1$aic)
cm
F1(cm)
# eliminate variables as they improve accuracy and F1 score
mod3 <- glm(target ~ . -chas -rm -zn, family = binomial(link = "logit"), train)
# summary(mod3)
pred.5 <- ifelse(predict.glm(mod3, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.5),
factor(eval$target),"1")
results <- tibble(model = "Model 3",
predictors = 1,
F1 = cm$byClass[7],
deviance=mod1$deviance,
r2 = 1 - mod1$deviance/mod1$null.deviance,
aic=mod1$aic)
cm
F1(cm)
# eliminate variables as they improve accuracy and F1 score
mod3 <- glm(target ~ . -chas -rm -nox, family = binomial(link = "logit"), train)
# summary(mod3)
pred.5 <- ifelse(predict.glm(mod3, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.5),
factor(eval$target),"1")
results <- tibble(model = "Model 3",
predictors = 1,
F1 = cm$byClass[7],
deviance=mod1$deviance,
r2 = 1 - mod1$deviance/mod1$null.deviance,
aic=mod1$aic)
cm
F1(cm)
# eliminate variables as they improve accuracy and F1 score
mod3 <- glm(target ~ . -chas -rm -age, family = binomial(link = "logit"), train)
# summary(mod3)
pred.5 <- ifelse(predict.glm(mod3, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.5),
factor(eval$target),"1")
results <- tibble(model = "Model 3",
predictors = 1,
F1 = cm$byClass[7],
deviance=mod1$deviance,
r2 = 1 - mod1$deviance/mod1$null.deviance,
aic=mod1$aic)
cm
F1(cm)
# eliminate variables as they improve accuracy and F1 score
mod3 <- glm(target ~ . -chas -rm -dis, family = binomial(link = "logit"), train)
# summary(mod3)
pred.5 <- ifelse(predict.glm(mod3, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.5),
factor(eval$target),"1")
results <- tibble(model = "Model 3",
predictors = 1,
F1 = cm$byClass[7],
deviance=mod1$deviance,
r2 = 1 - mod1$deviance/mod1$null.deviance,
aic=mod1$aic)
cm
F1(cm)
# eliminate variables as they improve accuracy and F1 score
mod3 <- glm(target ~ . -chas -rm -rad, family = binomial(link = "logit"), train)
# summary(mod3)
pred.5 <- ifelse(predict.glm(mod3, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.5),
factor(eval$target),"1")
results <- tibble(model = "Model 3",
predictors = 1,
F1 = cm$byClass[7],
deviance=mod1$deviance,
r2 = 1 - mod1$deviance/mod1$null.deviance,
aic=mod1$aic)
cm
F1(cm)
# eliminate variables as they improve accuracy and F1 score
mod3 <- glm(target ~ . -chas -rm -ptratio, family = binomial(link = "logit"), train)
# summary(mod3)
pred.5 <- ifelse(predict.glm(mod3, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.5),
factor(eval$target),"1")
results <- tibble(model = "Model 3",
predictors = 1,
F1 = cm$byClass[7],
deviance=mod1$deviance,
r2 = 1 - mod1$deviance/mod1$null.deviance,
aic=mod1$aic)
cm
F1(cm)
# eliminate variables as they improve accuracy and F1 score
mod3 <- glm(target ~ . -chas -rm -medv, family = binomial(link = "logit"), train)
# summary(mod3)
pred.5 <- ifelse(predict.glm(mod3, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.5),
factor(eval$target),"1")
results <- tibble(model = "Model 3",
predictors = 1,
F1 = cm$byClass[7],
deviance=mod1$deviance,
r2 = 1 - mod1$deviance/mod1$null.deviance,
aic=mod1$aic)
cm
F1(cm)
# eliminate variables as they improve accuracy and F1 score
mod3 <- glm(target ~ . -chas -rm, family = binomial(link = "logit"), train)
# summary(mod3)
pred.5 <- ifelse(predict.glm(mod3, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.5),
factor(eval$target),"1")
results <- tibble(model = "Model 3",
predictors = 1,
F1 = cm$byClass[7],
deviance=mod1$deviance,
r2 = 1 - mod1$deviance/mod1$null.deviance,
aic=mod1$aic)
cm
F1(cm)
# eliminate variables as they improve accuracy and F1 score
mod3 <- glm(target ~ . -chas -rm, family = binomial(link = "logit"), train)
summary(mod3)
pred.5 <- ifelse(predict.glm(mod3, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.5),
factor(eval$target),"1")
results <- tibble(model = "Model 3",
predictors = 1,
F1 = cm$byClass[7],
deviance=mod1$deviance,
r2 = 1 - mod1$deviance/mod1$null.deviance,
aic=mod1$aic)
cm
F1.mod3 <- F1(cm)
results
tbl(results)
kbl(results)
mod1 <- glm(target ~ medv, family = binomial(link = "logit"), training)
summary(mod1)
pred.1 <- ifelse(predict.glm(mod1, evalu,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.1),
factor(evalu$target),"1")
results <- tibble(model = "Model 1",
predictors = 1,
F1 = cm$byClass[7],
deviance=mod1$deviance,
r2 = 1 - mod1$deviance/mod1$null.deviance,
aic=mod1$aic)
cm
F1 <- function(c) {
sensitivity <- c$byClass[[1]]
precision <- c$byClass[[5]]
return( (2*precision*sensitivity) /
(precision + sensitivity))
}
F1.mod1 <- F1(cm)
mod1.all <- glm(target ~ ., family = binomial(link = "logit"), training)
summary(mod1.all)
pred.2 <- ifelse(predict.glm(mod1.all, evalu,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.2),
factor(evalu$target),"1")
results <- rbind(results, tibble(model = "Model 1 All",
predictors = 11,
F1 = cm$byClass[7],
deviance=mod1.all$deviance,
r2 = 1 - mod1.all$deviance/mod1.all$null.deviance,
aic=mod1.all$aic))
cm
F1.mod1.all <- F1(cm)
mod2 <- glm(target ~ medv, family = binomial(link = "logit"), train)
summary(mod2)
pred.3 <- ifelse(predict.glm(mod2, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.3),
factor(eval$target),"1")
results <- rbind(results, tibble(model = "Model 2",
predictors = 1,
F1 = cm$byClass[7],
deviance=mod2$deviance,
r2 = 1 - mod2$deviance/mod2$null.deviance,
aic=mod2$aic))
cm
F1.mod2 <- F1(cm)
mod2.all <- glm(target ~ ., family = binomial(link = "logit"), train)
summary(mod2.all)
pred.4 <- ifelse(predict.glm(mod2.all, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.4),
factor(eval$target),"1")
results <- rbind(results, tibble(model = "Model 2 All",
predictors = 12,
F1 = cm$byClass[7],
deviance=mod2.all$deviance,
r2 = 1 - mod2.all$deviance/mod2.all$null.deviance,
aic=mod2.all$aic))
cm
F1.mod2.all <- F1(cm)
# eliminate variables as they improve accuracy and F1 score
mod3 <- glm(target ~ . -chas -rm, family = binomial(link = "logit"), train)
summary(mod3)
pred.5 <- ifelse(predict.glm(mod3, eval,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(pred.5),
factor(eval$target),"1")
results <- rbind(results, tibble(model = "Model 3",
predictors = 10,
F1 = cm$byClass[7],
deviance=mod3$deviance,
r2 = 1 - mod3$deviance/mod3$null.deviance,
aic=mod3$aic))
cm
F1.mod3 <- F1(cm)
results
par(pty = "s")
roc(train[["target"]], pred.5, plot = TRUE, legacy.axes = TRUE, print.auc = TRUE)
roc(train[["target"]], mod3$data, plot = TRUE, legacy.axes = TRUE, print.auc = TRUE)
roc(train[["target"]], mod3$y, plot = TRUE, legacy.axes = TRUE, print.auc = TRUE)
par(pty = "s")
roc(train[["target"]], mod3$y, plot = TRUE, legacy.axes = TRUE, print.auc = TRUE)
roc(train[["target"]], pred.5, plot = TRUE, legacy.axes = TRUE, print.auc = TRUE)
par(pty = "s")
roc(train[["target"]], pred.5, plot = TRUE, legacy.axes = TRUE, print.auc = TRUE)
par(pty = "s")
roc(training[["target"]], pred.5, plot = TRUE, legacy.axes = TRUE, print.auc = TRUE)
mod3$R
mod3$y
roc(train[["target"]], mod3$y, plot = TRUE, legacy.axes = TRUE, print.auc = TRUE)
roc(train[["target"]], mod2.all$y, plot = TRUE, legacy.axes = TRUE, print.auc = TRUE)
par(pty = "s")
roc(train[["target"]], mod2.all$y, plot = TRUE, legacy.axes = TRUE, print.auc = TRUE)
knitr::opts_chunk$set(echo = TRUE)
dt <- read_csv('https://raw.githubusercontent.com/palmorezm/data621/main/HW2/classification-output-data.csv')
head(dt)
dt.conf.tbl <- dt %>%
select(class, scored.class) %>%
table()
colnames(dt.conf.tbl) <- c("Predicted -", "Predicted +")
row.names(dt.conf.tbl) <- c("Observed -", "Observed +")
kbl(dt.conf.tbl, booktabs = T, caption = "Confusion Matrix") %>%
kable_styling(latex_options = c("striped", "hold_position"),  full_width = F)
# dt.conf.tbl
# Calculating accuracy function
Ac <- function(x){
tp <- sum(x$class == 1 & x$scored.class == 1)
tn <- sum(x$class == 0 & x$scored.class == 0)
return((tp + tn)/nrow(x))
}
# Accuracy of this data set is shown below
Ac(dt)
# Classification Error Rate function
errt <- function(x){
total <- nrow(x)
fn <- sum(x$class == 1 & x$scored.class ==0)
fp <- sum(x$class == 0 & x$scored.class ==1)
return((fn+fp)/total)
}
# Error rate of this data set is shown below
errt(dt)
# Precision function
precision <- function(x){
tp <- sum(x$class == 1 & x$scored.class == 1)
fp <- sum(x$class == 0 & x$scored.class == 1)
return(tp/(tp + fp))
}
# Precision of this data set is shown below
precision(dt)
# Sensitivity function
recall <- function(x){
fn <- sum(x$class == 1 & x$scored.class ==0)
tp <- sum(x$class == 1 & x$scored.class ==1)
return(tp/(tp+fn))
}
# Sensitivity of this data set is shown below
recall(dt)
# Specificity function
specif <- function(x){
tn <- sum(x$class == 0 & x$scored.class == 0)
fp <- sum(x$class == 0 & x$scored.class == 1)
return(tn/(tn + fp))
}
# Specificity of this data set is shown below
specif(dt)
# F1 Score function
F1 <- function(x) {
tp <- sum(x$class == 1 & x$scored.class == 1)
fp <- sum(x$class == 0 & x$scored.class == 1)
fn <- sum(x$class == 1 & x$scored.class ==0)
precision <- (tp/(tp + fp))
sensitivity <- (tp/(tp+fn))
return( (2*precision*sensitivity) /
(precision + sensitivity))
}
# F1 Score of this data set is shown below
F1(dt)
ROC <- function (x, y) {
x <- x[order(y, decreasing=T)]
d <- data.frame(TPR=cumsum(x)/sum(x),
FPR=cumsum(!x)/sum(!x),
x)
auc <- ((sum(d$TPR * c(diff(d$FPR),0)) +
sum(c(diff(d$TPR),0) * c(diff(d$FPR),0))/2))
plot(d$FPR, d$TPR,
type = 'l',
xlim=c(0.0,1.0),
main = "Function - ROC Curve",
xlab = "1-Specificity",
ylab = "Sensitivity") +
abline(coef = c(0,1)) +
text(0.65, 0.45, labels = print(paste("AUC =", round(auc, 3))))
return(head(d, 10))
}
ROC(dt$class, dt$scored.probability)
dt
roc(train$target, pred.5)
pred.5
roc(train$target, mod3$data$target)
rocdf <- pROC::roc(train$target, mod3$data$target)
ROC(train$target, mod3$data$target)
rocdf <- pROC::roc(train$target, mod3$linear.predictors)
ROC(train$target, mod3$data$target)
mod3$R
data.frame(pred.5)
eval$target
ROC(eval$target, pred.5)
par(mfrow=c(1,2))
ROC(eval$target, pred.5)
ROC(eval$target, pred.4)
ROC(eval$target, pred.4, legacy.axis=TRUE)
ROC(eval$target, pred.4, plot = TRUE, legacy.axes = TRUE)
roc(eval$target, pred.4, plot = TRUE, legacy.axes = TRUE)
roc(eval$target, pred.4, plot = TRUE, legacy.axes = TRUE, main="Model 2 All")
roc(eval$target, pred.4, plot = TRUE, legacy.axes = FALSE, main="Model 2 All")
roc(eval$target, pred.4, legacy.axes = FALSE, main="Model 2 All")
roc(eval$target, pred.4, legacy.axes = FALSE, main="Model 2 All")
roc(eval$target, pred.4, legacy.axes = TRUE, main="Model 2 All")
roc(eval$target, pred.4, plot=TRUE, legacy.axes =FALSE, main="Model 2 All")
ROC(eval$target, pred.4, plot=TRUE, legacy.axes =FALSE, main="Model 2 All")
ROC(eval$target, pred.4, main="Model 2 All")
ROC(eval$target, pred.4) "Model 2 All")
ROC(eval$target, pred.4)
pROC::plot.roc(eval$target, pred.4)
pROC::plot.roc(eval$target, pred.4, print.auc=TRUE)
pROC::plot.roc(eval$target, pred.4, print.auc=TRUE, main="Model 2 All")
pROC::plot.roc(eval$target, pred.4, print.auc=TRUE, main="Model 2 All", legacy.axes=TRUE)
pROC::plot.roc(eval$target, pred.4, print.auc=TRUE, main="Model 2 All", legacy.axes=FALSE)
par(mfrow=c(1,2))
ROC(eval$target, pred.5, print.auc=TRUE, main="Model 3")
par(mfrow=c(1,2))
pROC::plot.roc(eval$target, pred.5, print.auc=TRUE, main="Model 3")
pROC::plot.roc(eval$target, pred.4, print.auc=TRUE, main="Model 2 All")
par(mfrow=c(1,2), pty="s")
pROC::plot.roc(eval$target, pred.5, print.auc=TRUE, main="Model 3")
pROC::plot.roc(eval$target, pred.4, print.auc=TRUE, main="Model 2 All")
par(mfrow=c(1,2), pty="l")
par(mfrow=c(1,2), pty="l")
par(mfrow=c(1,2), pty="l")
par(mfrow=c(1,2), pty="o")
par(mfrow=c(1,2), pty="o")
par(mfrow=c(1,2), type="o")
pROC::plot.roc(eval$target, pred.5, print.auc=TRUE, main="Model 3")
pROC::plot.roc(eval$target, pred.4, print.auc=TRUE, main="Model 2 All")
par(mfrow=c(1,2), type="b")
pROC::plot.roc(eval$target, pred.5, print.auc=TRUE, main="Model 3")
pROC::plot.roc(eval$target, pred.4, print.auc=TRUE, main="Model 2 All")
par(mfrow=c(1,2), type="l")
pROC::plot.roc(eval$target, pred.5, print.auc=TRUE, main="Model 3")
pROC::plot.roc(eval$target, pred.4, print.auc=TRUE, main="Model 2 All")
par(mfrow=c(1,2), type="l")
pROC::plot.roc(eval$target, pred.5, print.auc=TRUE, type="l", main="Model 3")
pROC::plot.roc(eval$target, pred.4, print.auc=TRUE, main="Model 2 All")
par(mfrow=c(1,2), type="l")
pROC::plot.roc(eval$target, pred.5, print.auc=TRUE, type="s", main="Model 3")
pROC::plot.roc(eval$target, pred.4, print.auc=TRUE, main="Model 2 All")
par(mfrow=c(1,2))
pROC::plot.roc(eval$target, pred.5, print.auc=TRUE, type="b", main="Model 3")
pROC::plot.roc(eval$target, pred.4, print.auc=TRUE, main="Model 2 All")
par(mfrow=c(1,2))
pROC::plot.roc(eval$target, pred.5, print.auc=TRUE, type="l", main="Model 3")
pROC::plot.roc(eval$target, pred.4, print.auc=TRUE, main="Model 2 All")
par(mfrow=c(1,2))
pROC::plot.roc(eval$target, pred.5, print.auc=TRUE, type="o", main="Model 3")
pROC::plot.roc(eval$target, pred.4, print.auc=TRUE, main="Model 2 All")
par(mfrow=c(1,2))
pROC::plot.roc(eval$target, pred.5, print.auc=TRUE, type="s", main="Model 3")
pROC::plot.roc(eval$target, pred.4, print.auc=TRUE, type="s", main="Model 2 All")
