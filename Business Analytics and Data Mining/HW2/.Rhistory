(precision + sensitivity))
}
# F1 Score of this data set is shown below
F1(dt)
ROC <- function(x,p, ...){
# "using a sequence of thresholds from 0-1 at 0.01 intervals"
for (i in seq(0,1,0.01)){
v <- data.frame(class = x, scored.class = if_else(p >= i,1,0), scored.probability = p)
if(!exists('Ptp') & !exists('Pfp')){
Ptp <- recall(v)
Pfp <- 1- specif(v)
} else{
Ptp <- c(Ptp,recall(v))
Pfp <- c(Pfp, 1- specif(v))
}
}
# return a list of vectors as a data frame
rdf <- data.frame(Ptp, Pfp) %>% arrange(Pfp)
# calculate area under the curve
AUC <- signif(sum(rdf$Ptp * c(diff(rdf$Pfp),0)) +
sum(c(diff(rdf$Ptp),0) * c(diff(rdf$Pfp),0))/2, digits=2)
# visualize
ggplot(,aes(Pfp, Ptp)) +
geom_line() +
geom_abline(intercept = 0, slope = 1) +
annotate("text",x=.55,y=.40,
label = paste("AUC = ", round(AUC, digits=3))) +
xlab("False Positive Rate") +
ylab("True Positive Rate") +
ggtitle("ROC Curve") +
theme_classic() +
theme(plot.title = element_text(hjust = 0.5))
}
ROC(dt$class, dt$scored.probability)
ROC <- function(x, y){
x <- x[order(y, decreasing=T)]
d <- data.frame(TPR=cumsum(x)/sum(x),
FPR=cumsum(!x)/sum(!x),
x)
return(d)
}
R <- ROC(dt$class, dt$scored.probability)
auc <- (sum(R$TPR * c(diff(R$FPR),0)) +
sum(c(diff(R$TPR),0) * c(diff(R$FPR),0))/2)
plot(R$FPR, R$TPR,
type = 'l',
xlim=c(0.0,1.0),
main = "Function - ROC Curve",
xlab = "1-Specificity",
ylab = "Sensitivity") +
abline(coef = c(0,1)) +
text(0.65, 0.45, labels = round(auc, 3))
ROC <- function (x, y) {
x <- x[order(y, decreasing=T)]
d <- data.frame(TPR=cumsum(x)/sum(x),
FPR=cumsum(!x)/sum(!x),
x)
auc <- ((sum(d$TPR * c(diff(d$FPR),0)) +
sum(c(diff(d$TPR),0) * c(diff(d$FPR),0))/2))
plot(d$FPR, d$TPR,
type = 'l',
xlim=c(0.0,1.0),
main = "Function - ROC Curve",
xlab = "1-Specificity",
ylab = "Sensitivity") +
abline(coef = c(0,1)) +
text(0.65, 0.45, labels = round(auc, 3))
return()
}
ROC(dt$class, dt$scored.probability)
conf.tbl <- c( round(Ac(dt),3),
round(errt(dt),3),
round(precision(dt),3),
round(recall(dt),3),
round(specif(dt),3),
round(F1(dt),3))
names(conf.tbl) <- c("Accuracy", "Classification Error", "Precision",
"Sensitivity", "Specificity", "F1 Score")
conf.tbl <-as.data.frame(conf.tbl)
names(conf.tbl)[1]<-'Scores'
# kbl(conf.tbl)%>%
#   kable_classic("hover", full_width = F)
conf.tbl
conf.mat.caret <- confusionMatrix(data = as.factor(dt$scored.class),
reference = as.factor(dt$class), positive = '0')
caret.sens <- conf.mat.caret$byClass["Sensitivity"]
caret.accu <- conf.mat.caret$overall["Accuracy"]
caret.spec <- conf.mat.caret$byClass["Specificity"]
caret.stats <- as.data.frame(cbind(
round(caret.accu, 3),
round(caret.sens, 3),
round(caret.spec, 3)
))
colnames(caret.stats) <- c("Accuracy", "Sensitivity", "Specificity")
row.names(caret.stats) <- "Scores"
# kbl(caret.stats)
tbl(caret.stats)
# kbl(caret.stats)
caret.stats
conf.mat.caret
rocdf <- pROC::roc(dt$class, dt$scored.probability)
rocauc <- auc(dt$class, dt$scored.probability)
par(mfrow = c(1, 2), pty = 's')
plot(1-rocdf$specificities, rocdf$sensitivities, type='s',
main = "pROC - ROC Curve",
xlab = "1-Specificity",
ylab = "Sensitivity") +
abline(c(0,1))+
text(0.65, 0.45, labels = round(rocauc, 3))
ROC(dt$class, dt$scored.probability)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(kableExtra)
library(caret)
library(pROC)
dt <- read_csv('https://raw.githubusercontent.com/palmorezm/data621/main/HW2/classification-output-data.csv')
head(dt)
dt.conf.tbl <- dt %>%
select(class, scored.class) %>%
table()
colnames(dt.conf.tbl) <- c("Predicted -", "Predicted +")
row.names(dt.conf.tbl) <- c("Observed -", "Observed +")
kbl(dt.conf.tbl, booktabs = T, caption = "Confusion Matrix") %>%
kable_styling(latex_options = c("striped", "hold_position"),
full_width = F)
# Calculating accuracy function
Ac <- function(x){
tp <- sum(x$class == 1 & x$scored.class == 1)
tn <- sum(x$class == 0 & x$scored.class == 0)
return((tp + tn)/nrow(x))
}
# Accuracy of this data set is shown below
Ac(dt)
# Classification Error Rate function
errt <- function(x){
total <- nrow(x)
fn <- sum(x$class == 1 & x$scored.class ==0)
fp <- sum(x$class == 0 & x$scored.class ==1)
return((fn+fp)/total)
}
# Error rate of this data set is shown below
errt(dt)
# Precision function
precision <- function(x){
tp <- sum(x$class == 1 & x$scored.class == 1)
fp <- sum(x$class == 0 & x$scored.class == 1)
return(tp/(tp + fp))
}
# Precision of this data set is shown below
precision(dt)
# Sensitivity function
recall <- function(x){
fn <- sum(x$class == 1 & x$scored.class ==0)
tp <- sum(x$class == 1 & x$scored.class ==1)
return(tp/(tp+fn))
}
# Sensitivity of this data set is shown below
recall(dt)
# Specificity function
specif <- function(x){
tn <- sum(x$class == 0 & x$scored.class == 0)
fp <- sum(x$class == 0 & x$scored.class == 1)
return(tn/(tn + fp))
}
# Specificity of this data set is shown below
specif(dt)
# F1 Score function
F1 <- function(x) {
tp <- sum(x$class == 1 & x$scored.class == 1)
fp <- sum(x$class == 0 & x$scored.class == 1)
fn <- sum(x$class == 1 & x$scored.class ==0)
precision <- (tp/(tp + fp))
sensitivity <- (tp/(tp+fn))
return( (2*precision*sensitivity) /
(precision + sensitivity))
}
# F1 Score of this data set is shown below
F1(dt)
ROC <- function(x,p, ...){
# "using a sequence of thresholds from 0-1 at 0.01 intervals"
for (i in seq(0,1,0.01)){
v <- data.frame(class = x, scored.class = if_else(p >= i,1,0), scored.probability = p)
if(!exists('Ptp') & !exists('Pfp')){
Ptp <- recall(v)
Pfp <- 1- specif(v)
} else{
Ptp <- c(Ptp,recall(v))
Pfp <- c(Pfp, 1- specif(v))
}
}
# return a list of vectors as a data frame
rdf <- data.frame(Ptp, Pfp) %>% arrange(Pfp)
# calculate area under the curve
AUC <- signif(sum(rdf$Ptp * c(diff(rdf$Pfp),0)) +
sum(c(diff(rdf$Ptp),0) * c(diff(rdf$Pfp),0))/2, digits=2)
# visualize
ggplot(,aes(Pfp, Ptp)) +
geom_line() +
geom_abline(intercept = 0, slope = 1) +
annotate("text",x=.55,y=.40,
label = paste("AUC = ", round(AUC, digits=3))) +
xlab("False Positive Rate") +
ylab("True Positive Rate") +
ggtitle("ROC Curve") +
theme_classic() +
theme(plot.title = element_text(hjust = 0.5))
}
ROC(dt$class, dt$scored.probability)
ROC <- function(x, y){
x <- x[order(y, decreasing=T)]
d <- data.frame(TPR=cumsum(x)/sum(x),
FPR=cumsum(!x)/sum(!x),
x)
return(d)
}
R <- ROC(dt$class, dt$scored.probability)
auc <- (sum(R$TPR * c(diff(R$FPR),0)) +
sum(c(diff(R$TPR),0) * c(diff(R$FPR),0))/2)
plot(R$FPR, R$TPR,
type = 'l',
xlim=c(0.0,1.0),
main = "Function - ROC Curve",
xlab = "1-Specificity",
ylab = "Sensitivity") +
abline(coef = c(0,1)) +
text(0.65, 0.45, labels = round(auc, 3))
ROC <- function (x, y) {
x <- x[order(y, decreasing=T)]
d <- data.frame(TPR=cumsum(x)/sum(x),
FPR=cumsum(!x)/sum(!x),
x)
auc <- ((sum(d$TPR * c(diff(d$FPR),0)) +
sum(c(diff(d$TPR),0) * c(diff(d$FPR),0))/2))
plot(d$FPR, d$TPR,
type = 'l',
xlim=c(0.0,1.0),
main = "Function - ROC Curve",
xlab = "1-Specificity",
ylab = "Sensitivity") +
abline(coef = c(0,1)) +
text(0.65, 0.45, labels = round(auc, 3))
return()
}
ROC(dt$class, dt$scored.probability)
conf.tbl <- c( round(Ac(dt),3),
round(errt(dt),3),
round(precision(dt),3),
round(recall(dt),3),
round(specif(dt),3),
round(F1(dt),3))
names(conf.tbl) <- c("Accuracy", "Classification Error", "Precision",
"Sensitivity", "Specificity", "F1 Score")
conf.tbl <-as.data.frame(conf.tbl)
names(conf.tbl)[1]<-'Scores'
# kbl(conf.tbl)%>%
#   kable_classic("hover", full_width = F)
conf.tbl
conf.mat.caret <- confusionMatrix(data = as.factor(dt$scored.class),
reference = as.factor(dt$class), positive = '0')
caret.sens <- conf.mat.caret$byClass["Sensitivity"]
caret.accu <- conf.mat.caret$overall["Accuracy"]
caret.spec <- conf.mat.caret$byClass["Specificity"]
caret.stats <- as.data.frame(cbind(
round(caret.accu, 3),
round(caret.sens, 3),
round(caret.spec, 3)
))
colnames(caret.stats) <- c("Accuracy", "Sensitivity", "Specificity")
row.names(caret.stats) <- "Scores"
# kbl(caret.stats)
caret.stats
conf.mat.caret
# kbl(dt.conf.tbl, booktabs = T, caption = "Confusion Matrix") %>%
#  kable_styling(latex_options = c("striped", "hold_position"),  full_width = F)
dt.conf.tbl
# kbl(dt.conf.tbl, booktabs = T, caption = "Confusion Matrix") %>%
#  kable_styling(latex_options = c("striped", "hold_position"),  full_width = F)
tbl(dt.conf.tbl)
dt.conf.tbl <- dt %>%
select(class, scored.class) %>%
table()
colnames(dt.conf.tbl) <- c("Predicted -", "Predicted +")
row.names(dt.conf.tbl) <- c("Observed -", "Observed +")
# kbl(dt.conf.tbl, booktabs = T, caption = "Confusion Matrix") %>%
#  kable_styling(latex_options = c("striped", "hold_position"),  full_width = F)
dt.conf.tbl
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(kableExtra)
library(caret)
library(pROC)
dt <- read_csv('https://raw.githubusercontent.com/palmorezm/data621/main/HW2/
classification-output-data.csv')
head(dt)
dt.conf.tbl <- dt %>%
select(class, scored.class) %>%
table()
dt <- read_csv('https://raw.githubusercontent.com/palmorezm/data621/main/HW2
/classification-output-data.csv')
head(dt)
dt <- read_csv('https://raw.githubusercontent.com/palmorezm/data621/main/HW2/classification-output-data.csv')
head(dt)
dt.conf.tbl <- dt %>%
select(class, scored.class) %>%
table()
colnames(dt.conf.tbl) <- c("Predicted -", "Predicted +")
row.names(dt.conf.tbl) <- c("Observed -", "Observed +")
kbl(dt.conf.tbl, booktabs = T, caption = "Confusion Matrix") %>%
kable_styling(latex_options = c("striped", "hold_position"),  full_width = F)
# dt.conf.tbl
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(kableExtra)
library(caret)
library(pROC)
dt <- read_csv('https://raw.githubusercontent.com/palmorezm/data621/main/HW2/classification-output-data.csv')
head(dt)
dt.conf.tbl <- dt %>%
select(class, scored.class) %>%
table()
colnames(dt.conf.tbl) <- c("Predicted -", "Predicted +")
row.names(dt.conf.tbl) <- c("Observed -", "Observed +")
kbl(dt.conf.tbl, booktabs = T, caption = "Confusion Matrix") %>%
kable_styling(latex_options = c("striped", "hold_position"),  full_width = F)
# dt.conf.tbl
# Calculating accuracy function
Ac <- function(x){
tp <- sum(x$class == 1 & x$scored.class == 1)
tn <- sum(x$class == 0 & x$scored.class == 0)
return((tp + tn)/nrow(x))
}
# Accuracy of this data set is shown below
Ac(dt)
# Classification Error Rate function
errt <- function(x){
total <- nrow(x)
fn <- sum(x$class == 1 & x$scored.class ==0)
fp <- sum(x$class == 0 & x$scored.class ==1)
return((fn+fp)/total)
}
# Error rate of this data set is shown below
errt(dt)
# Precision function
precision <- function(x){
tp <- sum(x$class == 1 & x$scored.class == 1)
fp <- sum(x$class == 0 & x$scored.class == 1)
return(tp/(tp + fp))
}
# Precision of this data set is shown below
precision(dt)
# Sensitivity function
recall <- function(x){
fn <- sum(x$class == 1 & x$scored.class ==0)
tp <- sum(x$class == 1 & x$scored.class ==1)
return(tp/(tp+fn))
}
# Sensitivity of this data set is shown below
recall(dt)
# Specificity function
specif <- function(x){
tn <- sum(x$class == 0 & x$scored.class == 0)
fp <- sum(x$class == 0 & x$scored.class == 1)
return(tn/(tn + fp))
}
# Specificity of this data set is shown below
specif(dt)
# F1 Score function
F1 <- function(x) {
tp <- sum(x$class == 1 & x$scored.class == 1)
fp <- sum(x$class == 0 & x$scored.class == 1)
fn <- sum(x$class == 1 & x$scored.class ==0)
precision <- (tp/(tp + fp))
sensitivity <- (tp/(tp+fn))
return( (2*precision*sensitivity) /
(precision + sensitivity))
}
# F1 Score of this data set is shown below
F1(dt)
ROC <- function (x, y) {
x <- x[order(y, decreasing=T)]
d <- data.frame(TPR=cumsum(x)/sum(x),
FPR=cumsum(!x)/sum(!x),
x)
auc <- ((sum(d$TPR * c(diff(d$FPR),0)) +
sum(c(diff(d$TPR),0) * c(diff(d$FPR),0))/2))
plot(d$FPR, d$TPR,
type = 'l',
xlim=c(0.0,1.0),
main = "Function - ROC Curve",
xlab = "1-Specificity",
ylab = "Sensitivity") +
abline(coef = c(0,1)) +
text(0.65, 0.45, labels = round(auc, 3))
return()
}
ROC(dt$class, dt$scored.probability)
ROC <- function (x, y) {
x <- x[order(y, decreasing=T)]
d <- data.frame(TPR=cumsum(x)/sum(x),
FPR=cumsum(!x)/sum(!x),
x)
auc <- ((sum(d$TPR * c(diff(d$FPR),0)) +
sum(c(diff(d$TPR),0) * c(diff(d$FPR),0))/2))
plot(d$FPR, d$TPR,
type = 'l',
xlim=c(0.0,1.0),
main = "Function - ROC Curve",
xlab = "1-Specificity",
ylab = "Sensitivity") +
abline(coef = c(0,1)) +
text(0.65, 0.45, labels = print(paste("AUC=", round(auc, 3)))
return()
ROC <- function (x, y) {
x <- x[order(y, decreasing=T)]
d <- data.frame(TPR=cumsum(x)/sum(x),
FPR=cumsum(!x)/sum(!x),
x)
auc <- ((sum(d$TPR * c(diff(d$FPR),0)) +
sum(c(diff(d$TPR),0) * c(diff(d$FPR),0))/2))
plot(d$FPR, d$TPR,
type = 'l',
xlim=c(0.0,1.0),
main = "Function - ROC Curve",
xlab = "1-Specificity",
ylab = "Sensitivity") +
abline(coef = c(0,1)) +
text(0.65, 0.45, labels = print(paste("AUC=", round(auc, 3))))
return()
}
ROC(dt$class, dt$scored.probability)
ROC <- function (x, y) {
x <- x[order(y, decreasing=T)]
d <- data.frame(TPR=cumsum(x)/sum(x),
FPR=cumsum(!x)/sum(!x),
x)
auc <- ((sum(d$TPR * c(diff(d$FPR),0)) +
sum(c(diff(d$TPR),0) * c(diff(d$FPR),0))/2))
plot(d$FPR, d$TPR,
type = 'l',
xlim=c(0.0,1.0),
main = "Function - ROC Curve",
xlab = "1-Specificity",
ylab = "Sensitivity") +
abline(coef = c(0,1)) +
text(0.65, 0.45, labels = print(paste("AUC =", round(auc, 3))))
return()
}
ROC(dt$class, dt$scored.probability)
ROC <- function (x, y) {
x <- x[order(y, decreasing=T)]
d <- data.frame(TPR=cumsum(x)/sum(x),
FPR=cumsum(!x)/sum(!x),
x)
auc <- ((sum(d$TPR * c(diff(d$FPR),0)) +
sum(c(diff(d$TPR),0) * c(diff(d$FPR),0))/2))
plot(d$FPR, d$TPR,
type = 'l',
xlim=c(0.0,1.0),
main = "Function - ROC Curve",
xlab = "1-Specificity",
ylab = "Sensitivity") +
abline(coef = c(0,1)) +
text(0.65, 0.45, labels = print(paste("AUC =", round(auc, 3))))
return(head(d, 3))
}
ROC(dt$class, dt$scored.probability)
ROC <- function (x, y) {
x <- x[order(y, decreasing=T)]
d <- data.frame(TPR=cumsum(x)/sum(x),
FPR=cumsum(!x)/sum(!x),
x)
auc <- ((sum(d$TPR * c(diff(d$FPR),0)) +
sum(c(diff(d$TPR),0) * c(diff(d$FPR),0))/2))
plot(d$FPR, d$TPR,
type = 'l',
xlim=c(0.0,1.0),
main = "Function - ROC Curve",
xlab = "1-Specificity",
ylab = "Sensitivity") +
abline(coef = c(0,1)) +
text(0.65, 0.45, labels = print(paste("AUC =", round(auc, 3))))
return(head(d, 10))
}
ROC(dt$class, dt$scored.probability)
conf.tbl <- c( round(Ac(dt),3),
round(errt(dt),3),
round(precision(dt),3),
round(recall(dt),3),
round(specif(dt),3),
round(F1(dt),3))
names(conf.tbl) <- c("Accuracy", "Classification Error", "Precision",
"Sensitivity", "Specificity", "F1 Score")
conf.tbl <-as.data.frame(conf.tbl)
names(conf.tbl)[1]<-'Scores'
kbl(conf.tbl)%>%
kable_styling(latex_options = c("striped", "hold_position"), full_width = F)
# conf.tbl
conf.mat.caret <- confusionMatrix(data = as.factor(dt$scored.class),
reference = as.factor(dt$class), positive = '0')
caret.sens <- conf.mat.caret$byClass["Sensitivity"]
caret.accu <- conf.mat.caret$overall["Accuracy"]
caret.spec <- conf.mat.caret$byClass["Specificity"]
caret.stats <- as.data.frame(cbind(
round(caret.accu, 3),
round(caret.sens, 3),
round(caret.spec, 3)
))
colnames(caret.stats) <- c("Accuracy", "Sensitivity", "Specificity")
row.names(caret.stats) <- "Scores"
kbl(caret.stats) %>%
kable_styling(latex_options = c("striped", "hold_position"), full_width = F)
# caret.stats
